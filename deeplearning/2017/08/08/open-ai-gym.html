
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Google Analytics js-->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-101872700-1', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- code highlight js / css-->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <!-- BootStrap js / css-->
    <link href="/bootstrap-3.3.2/css/bootstrap.css" rel="stylesheet">
    <!-- jQuery (부트스트랩의 자바스크립트 플러그인을 위해 필요합니다) -->
     <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
     <!-- 모든 컴파일된 플러그인을 포함합니다 (아래), 원하지 않는다면 필요한 각각의 파일을 포함하세요 -->
     <script src="/bootstrap-3.3.2/js/bootstrap.js"></script>

    <!-- JUPYTER NOTEBOOK js / css-->

     <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

     <link href="/jupyterstyle/jupyterstyle.css" rel="stylesheet">


  <!-- Semantic UI js / css-->
    <link rel="stylesheet" type="text/css" href="/semantic/semantic.css">
    <script
      src="https://code.jquery.com/jquery-3.1.1.min.js"
      integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8="
      crossorigin="anonymous"></script>
    <script src="/semantic/semantic.js"></script>




    <title>open-AI 의 gym (python package) 이용해 강화학습 훈련하기 1: Q-learning </title>

  </head>
  <body>

    <div class = "ui container">
<!---
      <div class="ui visible left vertical inverted sidebar labeled icon menu">
        <div class="item">
          shinjayne
        </div>
        <h5> <a class="item" href = "https://shinjayne.github.io/">Home</a> </h5>
        <h5> <a class="item" href = "https://shinjayne.github.io//category">Categories</a> </h5>
        <h5> <a class="item" href = "https://shinjayne.github.io//about">About</a> </h5>
      </div>
-->
      <div class="ui stackable grid">
        <div class="one wide column">

        </div>

        <div class="fourteen wide column">

            <div class="ui center aligned inverted segment" >

              <h1  class="ui inverted header" ></h1>
              <h1  class="ui inverted header" >shinjayne</h1>
              <h1  class="ui inverted header" ></h1>

              <div class="right item">
                <a class="ui inverted button" href = "https://shinjayne.github.io/">Home</a>
                <a class="ui inverted button" href = "https://shinjayne.github.io//category">Categories</a>
                <a class="ui inverted button" href = "https://shinjayne.github.io//projects">Projects</a>
              </div>
            </div>
        </div>

        <div class="one wide column">

        </div>

        <div class="one wide column">

</div>

<div class="fourteen wide column">
  <div class = "ui segment">
    <h1>open-AI 의 gym (python package) 이용해 강화학습 훈련하기 1: Q-learning </h1>

    <li>
      작성일 : 08 August 2017
    </li>
    <li>
      카테고리 : <a href = "https://shinjayne.github.io//category">DeepLearning</a>
    </li>
  </div>

  <div class = "ui segment">
    
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>open-AI 에서 파이썬 패키지로 제공하는 gym 을 이용하면 , 손쉽게 강화학습 환경을 구성할 수 있다.</p>
<p>gym package 를 이용해서 강화학습 훈련 환경을 만들어보고, Q-learning 이라는 강화학습 알고리즘에 대해 알아보고 적용시켜보자.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><h3 id="&#47785;&#52264;">&#47785;&#52264;<a class="anchor-link" href="#&#47785;&#52264;">&#182;</a></h3><p><a href="#1.-gym-package-이용하기">1. gym package 이용하기</a></p>
<p><a href="#2.-Q-learning-이란?">2. Q-learning 이란? </a></p>
<blockquote><p><a href="#Q-learning-의-학습-(Greedy,-Dummy">2-1. Q-learning 의 학습(Dummy Q learning)</a>)</p>
<p><a href="#Dummy-Q-learning-학습---python-code">2-2. Dummy Q-learning python code</a></p>
</blockquote>
<p><a href="#3.-완벽한-Q-learning-(-Dummy-Q-learning-의-문제-">3. 완벽한 Q-learning ( Dummy Q-learning 의 문제 )</a>)</p>
<blockquote><p><a href="#해결책-1-:--E-greedy">3-1. 해결책 1 :  E-greedy </a></p>
<p><a href="#해결책-2-:-add-Random-noise">3-2. 해결책 2 : add Random Noise</a></p>
<p><a href="#새로운-문제-:-여러-경로가-생긴다">3-3. 새로운 문제 : 여러 경로 </a></p>
<p><a href="#Q-learning-python-코드와-실행결과">3-4. 완벽한 Q-learning python code </a></p>
</blockquote>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/>
<br/></p>
<h1 id="1.-gym-package-&#51060;&#50857;&#54616;&#44592;">1. gym package &#51060;&#50857;&#54616;&#44592;<a class="anchor-link" href="#1.-gym-package-&#51060;&#50857;&#54616;&#44592;">&#182;</a></h1><p><hr/>
<br/></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>open-AI 에서 만든 gym 이란 파이썬 패키지를 이용하면 강화학습( Reinforcement Learning ) 훈련을 수행할 수 있는 Agent와 Environment 를 제공받을 수 있다.</p>
<blockquote><p><a href="https://gym.openai.com/">open-AI gym 홈페이지</a></p>
</blockquote>
<p>gym 을 간단하게 pip install 통해서 설치할 수 있다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># command line (bash)</span>
<span class="err">$</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">gym</span>
<span class="err">$</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">readchar</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>실제로 gym 을 사용해본다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># gym_example.py</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">gym.envs.registration</span> <span class="k">import</span> <span class="n">register</span>
<span class="kn">import</span> <span class="nn">readchar</span>


<span class="n">LEFT</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">DOWN</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">RIGHT</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">UP</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">arrow_keys</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;</span><span class="se">\x1b</span><span class="s1">[A&#39;</span> <span class="p">:</span> <span class="n">UP</span><span class="p">,</span>
    <span class="s1">&#39;</span><span class="se">\x1b</span><span class="s1">[B&#39;</span> <span class="p">:</span> <span class="n">DOWN</span><span class="p">,</span>
    <span class="s1">&#39;</span><span class="se">\x1b</span><span class="s1">[C&#39;</span> <span class="p">:</span> <span class="n">RIGHT</span><span class="p">,</span>
    <span class="s1">&#39;</span><span class="se">\x1b</span><span class="s1">[D&#39;</span> <span class="p">:</span> <span class="n">LEFT</span>
<span class="p">}</span>


<span class="n">register</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="s1">&#39;FrozenLake-v3&#39;</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s2">&quot;gym.envs.toy_text:FrozenLakeEnv&quot;</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;map_name&#39;</span><span class="p">:</span><span class="s1">&#39;4x4&#39;</span><span class="p">,</span><span class="s1">&#39;is_slippery&#39;</span><span class="p">:</span><span class="kc">False</span><span class="p">})</span>


<span class="sd">&#39;&#39;&#39;여기서부터 gym 코드의 시작이다. env 는 agent 가 활동할 수 있는 environment 이다.&#39;&#39;&#39;</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;FrozenLake-v3&quot;</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span> <span class="c1">#환경을 화면으로 출력</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">readchar</span><span class="o">.</span><span class="n">readkey</span><span class="p">()</span>  <span class="c1">#키보드 입력을 받는다 </span>
    
    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">arrow_keys</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Game aborted!&quot;</span><span class="p">)</span>
        <span class="k">break</span>
        
    <span class="n">action</span> <span class="o">=</span> <span class="n">arrow_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="c1">#에이젼트의 움직임</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1">#움직임에 따른 결과값들 </span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span> <span class="c1">#화면을 다시 출력</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;State:&quot;</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="s2">&quot;Action&quot;</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="s2">&quot;Reward:&quot;</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="s2">&quot;Info:&quot;</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span> <span class="c1">#도착하면 게임을 끝낸다. </span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Finished with reward&quot;</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="k">break</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>위와 같은 코드로, 내가 직접 게임을 진행해볼 수 있다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/>
<br/></p>
<h1 id="2.-Q-learning-&#51060;&#46976;?">2. Q-learning &#51060;&#46976;?<a class="anchor-link" href="#2.-Q-learning-&#51060;&#46976;?">&#182;</a></h1><p><hr/>
<br/></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/1.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>위의 gym-example.py 코드같은 environment 에서, agent 가 무작위로 방향을 결정하면 학습이 잘 되지 않는다.</p>
<p>시도 횟수는 엄청 많은데에 비해 reward는 성공할 때 한번만 지급되기 때문이다.</p>
<p>그에 대한 해결책이 바로 Q-learning 이다.</p>
<p>agent는 방향을 결정해야할 때마다 가상의 Q 에게 행보를 물어본다.</p>
<p>Q 는 agent 의 state를 보고 그가 action1 을 취하면 기대되는 reward1(quality1) 을 알려주고, action2 를 취하면 기대되는 reward2(quality2) 를 알려준다.</p>
<p>agent 는 Q의 도움을 받아 더 빠른 학습이 가능하다.</p>
<p>수식적으로는</p>
<p>Q(s, a) = π  처럼 쓸 수 있으며</p>
<p>현재 s(state)에서 취할수 있는 가장 큰 reward 인 max Q  max(Q(s,a)) 로</p>
<p>현재 s(state)에서 max(Q(s,a))로 가게 해주는 action 은 argmax(Q(s,a)) 로 표현한다.</p>
<p>argmax(Q(s,a))는 π*(s) 로도 표현하며 여기서 *은 optimal 함을 의미한다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/2.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Q-learning-&#51032;-&#54617;&#49845;-(Greedy,-Dummy)">Q-learning &#51032; &#54617;&#49845; (Greedy, Dummy)<a class="anchor-link" href="#Q-learning-&#51032;-&#54617;&#49845;-(Greedy,-Dummy)">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Q-learning 알고리즘에서 학습한다는 것은 아래 그림처럼 모두 0으로 초기화되어있는 모든 Q 값들을 하나씩 업데이트시킨다는 것과 같다.</p>
<p>모든 칸의 0은 각각의 Q(s,a) 값을 나타낸다.</p>
<p>도착지점으로 넘어갈 때에만 reward = 1 이 주어지고, 나머지 부분으로 넘어갈 때에는 reward = 0 이다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/6.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>우선 각 칸의 Q 값을 업데이트 하는 방법은 다음과같다.</p>
<p>Q(s,a) 의 값은 (다음칸의 Q 중 가장 큰 값 + reward) 로 표현해서 적는다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/3.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>처음에는 어디로 가든 reward 도 0이고 max Q(s', a') 도 0이다. 그래서 agent가 무작위로 방향을 결정한다. 그러다 우연히 아래 그림처럼 도착지점 바로 왼쪽칸에 도착했다.</p>
<p>이때 Q(s14, right) 의 값은 reward = 1 값 더하기 maxQ(s15,a') = 0 으로 1 로 업데이트된다.</p>
<p>이게 첫번째 학습이다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/5.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>한번 학습이 끝난 후 agent 는 다시 시작지점에서 출발한다. 역시나 대부분의 Q가 0으로 초기화되어있고, 주변의 reward 도 0이기 때문에 무작위로 나아간다.</p>
<p>그러다 다시 우연히 s13 칸에 도착했다고 하자.</p>
<p>이때 Q(s13,right) 를 계산해보았더니, reward = 0 이지만 max Q(s',a') 이 1이다.</p>
<p>이렇게 다시 Q(s13,right) = 1 로 업데이트하고 다시 agent 를 시작점으로 보낸다.</p>
<p><br/>
<br/></p>
<p>이런식으로 계속해서 학습을 하다보면 결국 아래 그림처럼 시작 지점의 Q 까지 학습이 되고, 이렇게 Dummy Q-learning 의 학습이 끝난다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/3.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>루틴을 정리하자면 다음과 같다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/4.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dummy-Q-learning-&#54617;&#49845;---python-code">Dummy Q-learning &#54617;&#49845; - python code<a class="anchor-link" href="#Dummy-Q-learning-&#54617;&#49845;---python-code">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dummy_q_learning.py</span>


<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">gym.envs.registration</span> <span class="k">import</span> <span class="n">register</span>
<span class="kn">import</span> <span class="nn">random</span> <span class="k">as</span> <span class="nn">pr</span>


<span class="k">def</span> <span class="nf">qmax_action</span><span class="p">(</span><span class="n">four_q</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; 상태 s 에서 네가지 a 에 따른 네가지 Q 중 가장 큰 것을 선택 (같으면 랜덤하게 선택)&quot;&quot;&quot;</span>
    <span class="n">maxq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">four_q</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">four_q</span> <span class="o">==</span> <span class="n">maxq</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">pr</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>


<span class="n">register</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="s1">&#39;FrozenLake-v3&#39;</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s1">&#39;gym.envs.toy_text:FrozenLakeEnv&#39;</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;map_name&#39;</span><span class="p">:</span> <span class="s1">&#39;4x4&#39;</span><span class="p">,</span>
            <span class="s1">&#39;is_slippery&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v3&#39;</span><span class="p">)</span>

<span class="c1"># shape = [States num, 4(left,down,right,up)]</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">])</span>
<span class="c1"># Set learning parameters</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="c1"># create lists to contain total rewards and steps per episode</span>
<span class="n">rList</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="c1"># Reset environment and get first new observation</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">rAll</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># The Q-Table learning algorithm</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">qmax_action</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:])</span>

        <span class="c1"># Get new state and reward from environment</span>
        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># Update Q-Table with new knowledge using learning rate</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">new_state</span><span class="p">,</span> <span class="p">:])</span>

        <span class="n">rAll</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

    <span class="n">rList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rAll</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Success rate: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rList</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_episodes</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Q-Table Values&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LEFT DOWN RIGHT UP&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rList</span><span class="p">)),</span> <span class="n">rList</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/>
<br/></p>
<h1 id="3.-&#50756;&#48317;&#54620;-Q-learning-(-Dummy-Q-learning-&#51032;-&#47928;&#51228;-)">3. &#50756;&#48317;&#54620; Q-learning ( Dummy Q-learning &#51032; &#47928;&#51228; )<a class="anchor-link" href="#3.-&#50756;&#48317;&#54620;-Q-learning-(-Dummy-Q-learning-&#51032;-&#47928;&#51228;-)">&#182;</a></h1><p><hr/>
<br/></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>dummy Q-learning 의 문제는 아래 그림처럼 가장 optimal 한 경로를 따라 Q 가 업데이트 되지 않을 수 있는 가능성이 있다는 것이다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/3_1.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>해결할 수 있는 방법은 단 하나다.</p>
<p>가끔은 최적의 Q로 이동하는 action 이 아닌 랜덤한 action을 취해주는 것이다.</p>
<p>(예를들어 위 그림에서 첫번째 state 일때, 오른쪽으로 가지 않고 한번 아래로 가보는 action을 취해보는 것이다.)</p>
<p>그 방법으로 2가지가 있다.</p>
<blockquote><ol>
<li><p>E-greedy (랜덤한 확률로 아무데나 가본다.)</p>
</li>
<li><p>add Random noise (Q 에 random noise 를 더해 랜덤한 action 을 취한다.)</p>
</li>
</ol>
</blockquote>
<p>위 두가지에 대해 자세히 알아보자.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#54644;&#44208;&#52293;-1-:--E-greedy">&#54644;&#44208;&#52293; 1 :  E-greedy<a class="anchor-link" href="#&#54644;&#44208;&#52293;-1-:--E-greedy">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/7.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>일정 확률로 가끔은 최적의 action 을 따라가지 않도록 설정한다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#54644;&#44208;&#52293;-2-:-add-Random-noise">&#54644;&#44208;&#52293; 2 : add Random noise<a class="anchor-link" href="#&#54644;&#44208;&#52293;-2-:-add-Random-noise">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/9.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>action 을 결정할 때 참고하는 각 Q 값에 random 한 noise 를 주어서 action 이 조금 random 해지도록  한다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#49352;&#47196;&#50868;-&#47928;&#51228;-:-&#50668;&#47084;-&#44221;&#47196;&#44032;-&#49373;&#44596;&#45796;">&#49352;&#47196;&#50868; &#47928;&#51228; : &#50668;&#47084; &#44221;&#47196;&#44032; &#49373;&#44596;&#45796;<a class="anchor-link" href="#&#49352;&#47196;&#50868;-&#47928;&#51228;-:-&#50668;&#47084;-&#44221;&#47196;&#44032;-&#49373;&#44596;&#45796;">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>위와 같이 dummy Q learning 문제를 보완하는 기법들을 사용하면 agent 가 최종적으로 경로를 결정하려고 할 때 선택의 문제에 놓인다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/10.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>이런 상황을 막으려면 학습과정에서 Q를 업데이트할 때, 다음 max Q(s', a') 값에다 특정 감마(\&lt;1) 값을 곱해준다.</p>
<p>그러면 상대적으로 구불하고 긴 경로로 인도하는 Q 값들은 작아지고, 가장 짧은 경로로 인도하는 Q 값들은 커진다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/11.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Q-learning-python-&#53076;&#46300;&#50752;-&#49892;&#54665;&#44208;&#44284;">Q-learning python &#53076;&#46300;&#50752; &#49892;&#54665;&#44208;&#44284;<a class="anchor-link" href="#Q-learning-python-&#53076;&#46300;&#50752;-&#49892;&#54665;&#44208;&#44284;">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># q_learning.py</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">gym.envs.registration</span> <span class="k">import</span> <span class="n">register</span>
<span class="kn">import</span> <span class="nn">random</span> <span class="k">as</span> <span class="nn">pr</span>

<span class="n">register</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="s1">&#39;FrozenLake-v3&#39;</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s1">&#39;gym.envs.toy_text:FrozenLakeEnv&#39;</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;map_name&#39;</span><span class="p">:</span> <span class="s1">&#39;4x4&#39;</span><span class="p">,</span>
            <span class="s1">&#39;is_slippery&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v3&#39;</span><span class="p">)</span>

<span class="c1"># Initialize table with all zeros</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">])</span>

<span class="sd">&#39;&#39;&#39;1. Q 값이 업데이트될 때 maxQ(s&#39;,a&#39;) 에 곱할 감마 값을 설정한다.&#39;&#39;&#39;</span>
<span class="n">dis</span> <span class="o">=</span> <span class="o">.</span><span class="mi">99</span>

<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="c1"># create lists to contain total rewards and steps per episode</span>
<span class="n">rList</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="c1"># Reset environment and get first new observation</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">rAll</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="sd">&#39;&#39;&#39;2. E-Greedy 를 위한 확률값을 만들어준다. (step i이 지남에 따라 decay 되도록 설정)&#39;&#39;&#39;</span>
    <span class="n">e</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">((</span><span class="n">i</span> <span class="o">//</span> <span class="mi">100</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  

    <span class="c1"># The Q-Table learning algorithm : 한번 수행할 때 마다 Q 한칸 업데이트</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        
        <span class="sd">&#39;&#39;&#39;E-Greedy 를 따라 작은 확률로 랜덤하게 가고, 큰 확률로 높은 Q 를 따르는 쪽으로 간다.&#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:])</span>

        <span class="c1"># Get new state and reward from environment</span>
        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># Update Q-Table with new knowledge using learning rate</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">dis</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">new_state</span><span class="p">,</span> <span class="p">:])</span>

        <span class="n">rAll</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

    <span class="n">rList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rAll</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Success rate: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rList</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_episodes</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Q-Table Values&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rList</span><span class="p">)),</span> <span class="n">rList</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/aigym/12.png" alt=""></p>

</div>
</div>
</div>
 


  </div>

</div>

<div class="one wide column">

</div>


        <div class="one wide column"></div>

        <div class="fourteen wide column">
          <hr />
          <div id="disqus_thread" class = "ui segment"></div>
          <script>

          /**
          *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
          *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
          /*
          var disqus_config = function () {
          this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
          this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
          };
          */
          (function() { // DON'T EDIT BELOW THIS LINE
          var d = document, s = d.createElement('script');
          s.src = 'https://shinjayne.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
          })();
          </script>
          <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

        </div>

        <div class="one wide column"></div>



        <div class="one wide column">
        </div>
        <div class="fourteen wide column">
          <hr />
          <p>
            powered by <a>Jupyter NoteBook</a> / <a>Jekyll</a> / <a>BootStrap</a> / <a>Semantic-UI</a> / <a>Github Pages</a>
          </p>
          <p>
            source code available on <a href="https://github.com/shinjayne/shinjayne.github.io">here</a>
          </p>
          <p>
            <h2 class=header>shinjayne.github.io</h2>
            <h3 class=header>developer blog</h2>
          </p>

        </div>
        <div class="one wide column">
        </div>



      </div>






    </div>
  </body>
</html>
