<!doctype html>
<html>
  <head>

    <title>not mnist 데이터 Neural Network | Jayne.who();</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!--bootstrap CSS-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb" crossorigin="anonymous">
    <!--code highlighter CSS for Jekyll Markdown to HTML Converter -->
    <link rel="stylesheet" href="/asset/static/pygments-codehighlight-css/vs.css" />
    <link rel="stylesheet" href="/asset/static/post_load.css" />
    <link rel="stylesheet" href="/asset/static/font/stylesheets/NotoSansKR-Hestia.css" />
    <style>
      body {
        font-family: 'Noto Sans Korean', sans-serif;
        font-weight: 350;
        padding-top: 50px;
      }
      h1,h2,h3,h4,h5,h6 {
        font-weight : 800;
        color : #148b8e;
      }
      .navbar, .nav {
        font-weight: 800;
      }
      a {
        color : #90b3d8;
      }

      .jaynewho-shadow-effect {
        box-shadow: 0px 0px 20px 11px rgba(0, 0, 0, 0.18);
      }
    </style>
    <!--Tawk.to Script-->
    <script src="/asset/static/tawk-chat-api.js"></script>
  </head>
  <body>

    

<nav id = "navbarbackground" class="navbar fixed-top navbar-expand-lg navbar-light" style="box-shadow: 0 3px 5px 0 rgba(0,0,0,0.1); background-color: white;">
  <!-- background-color : rgba(178, 85, 228, 0.94); background-image:url('/asset/media/image/gradient1.jpg');   background-blend-mode: color;
  background-size: cover; -->
  <a class="navbar-brand" href="/">
    <!-- <img src="/asset/media/image/logo.jpg" width="35" height="35" class="d-inline-block align-top border border-primary" alt="" style="border-radius:10px"> -->
    Jayne.who(<p id="logo_text" class="d-inline"></p>);
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav" style="font-weight : 600;">
      <li class="nav-item" id="navbar_profile">
        <a class="nav-link" href="/profile/">Profile </a>
      </li>
      <li class="nav-item" id="navbar_post">
        <a class="nav-link" href="/post/">Posts </a>
      </li>
      <li class="nav-item" id="navbar_project">
        <a class="nav-link" href="/project/">Projects </a>
      </li>
    </ul>
  </div>
</nav>

    <style>
/*post titles style*/
.post h1,h2,h3,h4,h5,h6 {
  font-weight: 700;
}
.post h1 {font-size: 1.7rem;margin-top: 4.5rem; padding-bottom: 10px; margin-bottom:1rem;  color:#213b80; border-bottom: 1px solid #213b8026;}
.post h2 {font-size: 1.4rem;margin-top: 3rem; color:#213b80;}
.post h3 {font-size: 1.2rem;margin-top: 2rem;  color:#213b80;}
.post h4 {font-size: 1.0rem;margin-top: 2rem; color:#213b80;}
.post h5 {font-size: 0.9rem;margin-top: 1.5rem;  color:#213b80;}
/*for image*/
.post img {
  max-width: 100%;
  height: auto;
  margin-top : 2rem ;
  margin-bottom : 2rem;
  border-radius: 5px;
  box-shadow: 0px 0px 20px 6px rgba(0, 0, 0, 0.18);
}

/*for code highlighter*/
pre {
  background-color: #dddddd8c ;
  border-radius: 6px;
  padding: 10px;
  /* color : #d0d7de; */
}

/*for every text in different line*/
p {
  margin-bottom: 8px;
}

/*Table Form*/
th{
  white-space: nowrap;
}

</style>

<div class="container-fluid text-center pt-3" style="background-image : url('');  background-color: #00010299; background-blend-mode: color; background-size: cover; min-height: 350px;">
  <a href="/post/"><p class="text-left md-5"><button type="button" class="btn btn-outline-secondary btn-sm d-inline text-uppercase"> < Back </button></p></a>
  <h1 class="card-title text-white" style="font-size : 2.2rem;">not mnist 데이터 Neural Network</h1>
  <p class="card-text text-white"><p class="text-muted">deeplearning | 01 June 2017</p></p>
  <p class="card-text text-white">
    Tags |
    
    <a href="#" class="badge badge-primary">python</a>
    
    <a href="#" class="badge badge-primary">tensorflow</a>
    
  </p>
</div>


<div class="post container pt-5 " style = "max-width : 750px"><h1 id="deep-learning">Deep Learning</h1>

<h2 id="assignment-1">Assignment 1</h2>

<p>The objective of this assignment is to learn about simple data curation practices, and familiarize you with some of the data we’ll be reusing later.</p>

<p>This notebook uses the <a href="http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html">notMNIST</a> dataset to be used with python experiments. This dataset is designed to look like the classic <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> dataset, while looking a little more like real data: it’s a harder task, and the data is a lot less ‘clean’ than MNIST.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># These are all the modules we'll be using later. Make sure you can import them</span>
<span class="c"># before proceeding further.</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tarfile</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">ndimage</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">six.moves.urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="n">cPickle</span> <span class="k">as</span> <span class="n">pickle</span>

<span class="c"># Config the matplotlib backend as plotting inline in IPython</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre>
</div>

<p>First, we’ll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to ‘A’ through ‘J’ (10 classes). The training set has about 500k and the testset 19000 labeled examples. Given these sizes, it should be possible to train models quickly on any machine.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">url</span> <span class="o">=</span> <span class="s">'https://commondatastorage.googleapis.com/books1000/'</span>
<span class="n">last_percent_reported</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">data_root</span> <span class="o">=</span> <span class="s">'.'</span> <span class="c"># Change me to store data elsewhere</span>

<span class="k">def</span> <span class="nf">download_progress_hook</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">blockSize</span><span class="p">,</span> <span class="n">totalSize</span><span class="p">):</span>
  <span class="s">"""A hook to report the progress of a download. This is mostly intended for users with
  slow internet connections. Reports every 5</span><span class="si">% </span><span class="s">change in download progress.
  """</span>
  <span class="k">global</span> <span class="n">last_percent_reported</span>
  <span class="n">percent</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">count</span> <span class="o">*</span> <span class="n">blockSize</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="n">totalSize</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">last_percent_reported</span> <span class="o">!=</span> <span class="n">percent</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">percent</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">"</span><span class="si">%</span><span class="s">s</span><span class="si">%%</span><span class="s">"</span> <span class="o">%</span> <span class="n">percent</span><span class="p">)</span>
      <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">"."</span><span class="p">)</span>
      <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

    <span class="n">last_percent_reported</span> <span class="o">=</span> <span class="n">percent</span>

<span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">,</span> <span class="n">force</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="s">"""Download a file if not present, and make sure it's the right size."""</span>
  <span class="n">dest_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_root</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">force</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">dest_filename</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Attempting to download:'</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span> <span class="o">+</span> <span class="n">filename</span><span class="p">,</span> <span class="n">dest_filename</span><span class="p">,</span> <span class="n">reporthook</span><span class="o">=</span><span class="n">download_progress_hook</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Download Complete!'</span><span class="p">)</span>
  <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">stat</span><span class="p">(</span><span class="n">dest_filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Found and verified'</span><span class="p">,</span> <span class="n">dest_filename</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span>
      <span class="s">'Failed to verify '</span> <span class="o">+</span> <span class="n">dest_filename</span> <span class="o">+</span> <span class="s">'. Can you get to it with a browser?'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">dest_filename</span>

<span class="n">train_filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s">'notMNIST_large.tar.gz'</span><span class="p">,</span> <span class="mi">247336696</span><span class="p">)</span>
<span class="n">test_filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s">'notMNIST_small.tar.gz'</span><span class="p">,</span> <span class="mi">8458043</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Found and verified ./notMNIST_large.tar.gz
Found and verified ./notMNIST_small.tar.gz
</code></pre>
</div>

<p>Extract the dataset from the compressed .tar.gz file.
This should give you a set of directories, labeled A through J.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">maybe_extract</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">force</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="n">root</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">filename</span><span class="p">)[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>  <span class="c"># remove .tar.gz</span>
  <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">root</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">force</span><span class="p">:</span>
    <span class="c"># You may override by setting force=True.</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">s already present - Skipping extraction of </span><span class="si">%</span><span class="s">s.'</span> <span class="o">%</span> <span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Extracting data for </span><span class="si">%</span><span class="s">s. This may take a while. Please wait.'</span> <span class="o">%</span> <span class="n">root</span><span class="p">)</span>
    <span class="n">tar</span> <span class="o">=</span> <span class="n">tarfile</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
    <span class="n">tar</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">data_root</span><span class="p">)</span>
    <span class="n">tar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
  <span class="n">data_folders</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">root</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">d</span><span class="p">))]</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_folders</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_classes</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span>
      <span class="s">'Expected </span><span class="si">%</span><span class="s">d folders, one per class. Found </span><span class="si">%</span><span class="s">d instead.'</span> <span class="o">%</span> <span class="p">(</span>
        <span class="n">num_classes</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_folders</span><span class="p">)))</span>
  <span class="k">print</span><span class="p">(</span><span class="n">data_folders</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">data_folders</span>

<span class="n">train_folders</span> <span class="o">=</span> <span class="n">maybe_extract</span><span class="p">(</span><span class="n">train_filename</span><span class="p">)</span>
<span class="n">test_folders</span> <span class="o">=</span> <span class="n">maybe_extract</span><span class="p">(</span><span class="n">test_filename</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>./notMNIST_large already present - Skipping extraction of ./notMNIST_large.tar.gz.
['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']
./notMNIST_small already present - Skipping extraction of ./notMNIST_small.tar.gz.
['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']
</code></pre>
</div>

<hr />
<p>Problem 1
———</p>

<p>Let’s take a peek at some of the data to make sure it looks sensible. Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display.</p>

<hr />

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">png_list</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">test_folders</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="p">:</span>
        <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">test_folders</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="s">'/'</span><span class="o">+</span><span class="n">png_list</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="mi">100</span><span class="p">]))</span>

</code></pre>
</div>

<p><img src="output_7_0.png" alt="png" /></p>

<p><img src="output_7_1.png" alt="png" /></p>

<p><img src="output_7_2.png" alt="png" /></p>

<p><img src="output_7_3.png" alt="png" /></p>

<p><img src="output_7_4.png" alt="png" /></p>

<p><img src="output_7_5.png" alt="png" /></p>

<p><img src="output_7_6.png" alt="png" /></p>

<p><img src="output_7_7.png" alt="png" /></p>

<p><img src="output_7_8.png" alt="png" /></p>

<p><img src="output_7_9.png" alt="png" /></p>

<p><img src="output_7_10.png" alt="png" /></p>

<p><img src="output_7_11.png" alt="png" /></p>

<p><img src="output_7_12.png" alt="png" /></p>

<p><img src="output_7_13.png" alt="png" /></p>

<p><img src="output_7_14.png" alt="png" /></p>

<p><img src="output_7_15.png" alt="png" /></p>

<p><img src="output_7_16.png" alt="png" /></p>

<p><img src="output_7_17.png" alt="png" /></p>

<p><img src="output_7_18.png" alt="png" /></p>

<p><img src="output_7_19.png" alt="png" /></p>

<p><img src="output_7_20.png" alt="png" /></p>

<p><img src="output_7_21.png" alt="png" /></p>

<p><img src="output_7_22.png" alt="png" /></p>

<p><img src="output_7_23.png" alt="png" /></p>

<p><img src="output_7_24.png" alt="png" /></p>

<p><img src="output_7_25.png" alt="png" /></p>

<p><img src="output_7_26.png" alt="png" /></p>

<p><img src="output_7_27.png" alt="png" /></p>

<p><img src="output_7_28.png" alt="png" /></p>

<p><img src="output_7_29.png" alt="png" /></p>

<p><img src="output_7_30.png" alt="png" /></p>

<p><img src="output_7_31.png" alt="png" /></p>

<p><img src="output_7_32.png" alt="png" /></p>

<p><img src="output_7_33.png" alt="png" /></p>

<p><img src="output_7_34.png" alt="png" /></p>

<p><img src="output_7_35.png" alt="png" /></p>

<p><img src="output_7_36.png" alt="png" /></p>

<p><img src="output_7_37.png" alt="png" /></p>

<p><img src="output_7_38.png" alt="png" /></p>

<p><img src="output_7_39.png" alt="png" /></p>

<p><img src="output_7_40.png" alt="png" /></p>

<p><img src="output_7_41.png" alt="png" /></p>

<p><img src="output_7_42.png" alt="png" /></p>

<p><img src="output_7_43.png" alt="png" /></p>

<p><img src="output_7_44.png" alt="png" /></p>

<p><img src="output_7_45.png" alt="png" /></p>

<p><img src="output_7_46.png" alt="png" /></p>

<p><img src="output_7_47.png" alt="png" /></p>

<p><img src="output_7_48.png" alt="png" /></p>

<p><img src="output_7_49.png" alt="png" /></p>

<p>Now let’s load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we’ll load each class into a separate dataset, store them on disk and curate them independently. Later we’ll merge them into a single dataset of manageable size.</p>

<p>We’ll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road.</p>

<p>A few images might not be readable, we’ll just skip them.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">image_size</span> <span class="o">=</span> <span class="mi">28</span>  <span class="c"># Pixel width and height.</span>
<span class="n">pixel_depth</span> <span class="o">=</span> <span class="mf">255.0</span>  <span class="c"># Number of levels per pixel.</span>

<span class="k">def</span> <span class="nf">load_letter</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">min_num_images</span><span class="p">):</span>
  <span class="s">"""Load the data for a single letter label."""</span>
  <span class="n">image_files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">image_files</span><span class="p">),</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">),</span>
                         <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span>
  <span class="n">num_images</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">image_files</span><span class="p">:</span>
    <span class="n">image_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">image_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">ndimage</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">image_file</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="o">-</span>
                    <span class="n">pixel_depth</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">pixel_depth</span>
      <span class="k">if</span> <span class="n">image_data</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">'Unexpected image shape: </span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">image_data</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
      <span class="n">dataset</span><span class="p">[</span><span class="n">num_images</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">image_data</span>
      <span class="n">num_images</span> <span class="o">=</span> <span class="n">num_images</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">except</span> <span class="nb">IOError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'Could not read:'</span><span class="p">,</span> <span class="n">image_file</span><span class="p">,</span> <span class="s">':'</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="s">'- it</span><span class="se">\'</span><span class="s">s ok, skipping.'</span><span class="p">)</span>

  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_images</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
  <span class="k">if</span> <span class="n">num_images</span> <span class="o">&lt;</span> <span class="n">min_num_images</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">'Many fewer images than expected: </span><span class="si">%</span><span class="s">d &lt; </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span>
                    <span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="n">min_num_images</span><span class="p">))</span>

  <span class="k">print</span><span class="p">(</span><span class="s">'Full dataset tensor:'</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Mean:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Standard deviation:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">dataset</span>

<span class="k">def</span> <span class="nf">maybe_pickle</span><span class="p">(</span><span class="n">data_folders</span><span class="p">,</span> <span class="n">min_num_images_per_class</span><span class="p">,</span> <span class="n">force</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="n">dataset_names</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">folder</span> <span class="ow">in</span> <span class="n">data_folders</span><span class="p">:</span>
    <span class="n">set_filename</span> <span class="o">=</span> <span class="n">folder</span> <span class="o">+</span> <span class="s">'.pickle'</span>
    <span class="n">dataset_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">set_filename</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">set_filename</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">force</span><span class="p">:</span>
      <span class="c"># You may override by setting force=True.</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">s already present - Skipping pickling.'</span> <span class="o">%</span> <span class="n">set_filename</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'Pickling </span><span class="si">%</span><span class="s">s.'</span> <span class="o">%</span> <span class="n">set_filename</span><span class="p">)</span>
      <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_letter</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">min_num_images_per_class</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">set_filename</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
          <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>
      <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Unable to save data to'</span><span class="p">,</span> <span class="n">set_filename</span><span class="p">,</span> <span class="s">':'</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">dataset_names</span>

<span class="n">train_datasets</span> <span class="o">=</span> <span class="n">maybe_pickle</span><span class="p">(</span><span class="n">train_folders</span><span class="p">,</span> <span class="mi">45000</span><span class="p">)</span>
<span class="n">test_datasets</span> <span class="o">=</span> <span class="n">maybe_pickle</span><span class="p">(</span><span class="n">test_folders</span><span class="p">,</span> <span class="mi">1800</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>./notMNIST_large/A.pickle already present - Skipping pickling.
./notMNIST_large/B.pickle already present - Skipping pickling.
./notMNIST_large/C.pickle already present - Skipping pickling.
./notMNIST_large/D.pickle already present - Skipping pickling.
./notMNIST_large/E.pickle already present - Skipping pickling.
./notMNIST_large/F.pickle already present - Skipping pickling.
./notMNIST_large/G.pickle already present - Skipping pickling.
./notMNIST_large/H.pickle already present - Skipping pickling.
./notMNIST_large/I.pickle already present - Skipping pickling.
./notMNIST_large/J.pickle already present - Skipping pickling.
./notMNIST_small/A.pickle already present - Skipping pickling.
./notMNIST_small/B.pickle already present - Skipping pickling.
./notMNIST_small/C.pickle already present - Skipping pickling.
./notMNIST_small/D.pickle already present - Skipping pickling.
./notMNIST_small/E.pickle already present - Skipping pickling.
./notMNIST_small/F.pickle already present - Skipping pickling.
./notMNIST_small/G.pickle already present - Skipping pickling.
./notMNIST_small/H.pickle already present - Skipping pickling.
./notMNIST_small/I.pickle already present - Skipping pickling.
./notMNIST_small/J.pickle already present - Skipping pickling.
</code></pre>
</div>

<hr />
<p>Problem 2
———</p>

<p>Let’s verify that the data still looks good. Displaying a sample of the labels and images from the ndarray. Hint: you can use matplotlib.pyplot.</p>

<hr />

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">pro2_dataset</span> <span class="o">=</span> <span class="n">load_letter</span><span class="p">(</span><span class="n">test_folders</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>./notMNIST_small/A
Could not read: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file './notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.
Full dataset tensor: (1872, 28, 28)
Mean: -0.132626
Standard deviation: 0.445128
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pro2_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">pro2_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>[[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5        -0.5        -0.45294118 -0.45686275 -0.49607843
  -0.5        -0.46470588 -0.48039216 -0.5        -0.49607843 -0.5        -0.5
  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5        -0.5        -0.44117647 -0.40980393 -0.17843138
  -0.04509804 -0.26078433 -0.1509804  -0.35882354 -0.5        -0.49607843
  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.5        -0.49215686 -0.5        -0.2254902   0.11960784  0.26862746
   0.5         0.37058824  0.38627452 -0.01372549 -0.37843138 -0.49215686
  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.49215686 -0.5        -0.32745099  0.06862745  0.30000001  0.5
   0.48039216  0.49215686  0.5         0.39411765  0.04509804 -0.5
  -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.49215686 -0.5        -0.31960785  0.03333334  0.44509804  0.49607843
   0.49607843  0.49215686  0.49607843  0.46078432  0.13137256 -0.3392157
  -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.48039216 -0.5        -0.17058824  0.49607843  0.49607843  0.49607843
   0.5         0.5         0.5         0.44117647  0.4137255   0.00588235
  -0.5        -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.49215686 -0.5        -0.20196079  0.20980392  0.40588236  0.48823529
   0.5         0.5         0.5         0.49607843  0.5         0.45686275
   0.01764706 -0.37843138 -0.5        -0.49607843 -0.5        -0.5        -0.5
  -0.5        -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.48823529 -0.5         0.00980392  0.20588236  0.45686275  0.5
   0.48823529  0.48823529  0.5         0.49215686  0.5         0.35882354
   0.36666667 -0.26862746 -0.5        -0.48823529 -0.5        -0.5        -0.5
  -0.5        -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.48431373 -0.45294118  0.04117647  0.36666667  0.5         0.49607843
   0.49215686  0.47254902  0.4254902   0.49215686  0.49607843  0.5
   0.42156863 -0.06078431 -0.5        -0.49215686 -0.5        -0.5        -0.5
  -0.5        -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.5        -0.49607843
  -0.5        -0.31568629  0.23333333  0.5         0.49215686  0.5
   0.45294118  0.21764706  0.23333333  0.30000001  0.5         0.49215686
   0.49607843  0.06862745 -0.22941177 -0.5        -0.49607843 -0.5        -0.5
  -0.5        -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.49607843 -0.5        -0.44901961
  -0.37450981  0.17058824  0.31176472  0.5         0.49215686  0.5
   0.2764706  -0.43333334 -0.10392157  0.22156863  0.44901961  0.5         0.5
   0.37843138  0.04901961 -0.34705883 -0.5        -0.49215686 -0.5        -0.5
  -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.49607843 -0.5        -0.36666667
  -0.46078432  0.04117647  0.5         0.48431373  0.49607843  0.49215686
   0.0254902  -0.49215686 -0.23333333  0.1627451   0.46078432  0.5         0.5
   0.46862745  0.10784314 -0.22941177 -0.5        -0.49215686 -0.5        -0.5
  -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.48823529 -0.43333334
  -0.1509804   0.4254902   0.44901961  0.5         0.5         0.33137256
  -0.06078431 -0.42156863 -0.48431373 -0.09607843  0.24509804  0.37450981
   0.5         0.5         0.37450981 -0.20196079 -0.44117647 -0.5
  -0.49607843 -0.5        -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.48823529 -0.41764706
   0.18627451  0.48823529  0.49215686  0.49607843  0.42941177  0.0882353
  -0.28823531 -0.5        -0.49215686 -0.36274511  0.1509804   0.45686275
   0.49607843  0.5         0.41764706  0.05294118 -0.26862746 -0.5
  -0.49607843 -0.5        -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.49607843 -0.5        -0.19803922
   0.40588236  0.5         0.5         0.49607843  0.36274511  0.0372549
  -0.3509804  -0.5        -0.46862745 -0.37450981 -0.10784314  0.47254902
   0.49215686  0.49215686  0.5         0.15490197 -0.37843138 -0.40196079
  -0.48431373 -0.5        -0.5        -0.5       ]
 [-0.5        -0.5        -0.5        -0.5        -0.38627452 -0.21372549
   0.24117647  0.5         0.5         0.42941177  0.21764706 -0.19019608
  -0.5        -0.49215686 -0.5        -0.44509804 -0.17058824  0.12352941
   0.5         0.49215686  0.48823529  0.5        -0.14705883 -0.44117647
  -0.46470588 -0.5        -0.5        -0.5       ]
 [-0.5        -0.5        -0.49607843 -0.48823529 -0.33137256  0.17450981
   0.46470588  0.49607843  0.5         0.46470588  0.29215688  0.06078431
  -0.11960784 -0.26078433 -0.04509804 -0.17450981 -0.12745099  0.38235295
   0.42941177  0.49215686  0.5         0.47647059  0.22941177 -0.3509804
  -0.48431373 -0.5        -0.5        -0.5       ]
 [-0.5        -0.49607843 -0.5        -0.42156863 -0.1627451   0.35882354
   0.49607843  0.5         0.5         0.47254902  0.44117647  0.30784315
   0.24117647  0.42941177  0.18627451  0.28823531  0.35882354  0.4254902
   0.45294118  0.49607843  0.5         0.48039216  0.43333334 -0.21372549
  -0.4254902  -0.5        -0.49607843 -0.5       ]
 [-0.5        -0.48823529 -0.48823529 -0.31568629  0.10392157  0.42156863
   0.5         0.49607843  0.49607843  0.5         0.5         0.5         0.5
   0.49607843  0.46078432  0.49215686  0.47647059  0.5         0.5         0.5
   0.5         0.48823529  0.5         0.16666667 -0.36666667 -0.48431373
  -0.5        -0.5       ]
 [-0.5        -0.48431373 -0.48431373 -0.07254902  0.38627452  0.39411765
   0.5         0.49215686  0.5         0.49607843  0.47647059  0.48823529
   0.5         0.49215686  0.49607843  0.49215686  0.5         0.5
   0.49215686  0.49215686  0.5         0.48823529  0.5         0.33137256
  -0.33137256 -0.45294118 -0.5        -0.5       ]
 [-0.49607843 -0.5        -0.43333334 -0.00196078  0.47254902  0.45686275
   0.5         0.5         0.48431373  0.48039216  0.48039216  0.48431373
   0.4137255   0.49215686  0.40980393  0.44901961  0.38627452  0.46862745
   0.48823529  0.5         0.49607843  0.49607843  0.5         0.36666667
  -0.16666667 -0.4137255  -0.5        -0.49607843]
 [-0.5        -0.47254902 -0.14313726  0.21764706  0.47647059  0.5
   0.49607843  0.47647059  0.06470589 -0.03333334  0.26862746 -0.04509804
  -0.17058824 -0.10392157 -0.17058824  0.06470589  0.1        -0.19803922
  -0.02156863  0.31176472  0.46470588  0.45294118  0.5         0.49607843
   0.26078433 -0.36274511 -0.5        -0.49215686]
 [-0.48431373 -0.33529413 -0.04901961  0.33137256  0.5         0.48431373
   0.5         0.13529412 -0.21372549 -0.36274511 -0.5        -0.46078432
  -0.44509804 -0.5        -0.43725491 -0.47647059 -0.36666667 -0.4254902
  -0.47647059 -0.01764706  0.23333333  0.38235295  0.5         0.49215686
   0.31960785  0.15490197 -0.5        -0.49607843]
 [-0.44901961 -0.30784315  0.09607843  0.48039216  0.49607843  0.5
   0.43333334  0.00980392 -0.29215688 -0.47254902 -0.5        -0.49215686
  -0.49607843 -0.48431373 -0.49607843 -0.49607843 -0.5        -0.49215686
  -0.5        -0.43725491  0.02156863  0.44509804  0.5         0.5
   0.4254902   0.07254902 -0.11960784 -0.48431373]
 [-0.39803922 -0.12745099  0.3509804   0.5         0.48823529  0.43725491
   0.19019608 -0.11176471 -0.43725491 -0.5        -0.49607843 -0.5        -0.5
  -0.5        -0.5        -0.5        -0.49607843 -0.49607843 -0.48823529
  -0.5        -0.25686276  0.39803922  0.42156863  0.49607843  0.48039216
   0.36274511  0.00588235 -0.5       ]
 [-0.33137256  0.21764706  0.48431373  0.5         0.4254902   0.44117647
  -0.02156863 -0.40980393 -0.44509804 -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5        -0.5        -0.5        -0.5        -0.49607843
  -0.5        -0.41764706  0.1627451   0.44901961  0.45294118  0.5         0.5
  -0.00980392 -0.34313726]
 [-0.33137256 -0.02941176 -0.00588235  0.08431373  0.33137256  0.02156863
  -0.40980393 -0.49215686 -0.49215686 -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5        -0.5        -0.5        -0.5        -0.49607843
  -0.5        -0.35490197 -0.2372549   0.19803922  0.29607844  0.01372549
   0.20196079 -0.17843138 -0.45294118]
 [-0.48431373 -0.40196079 -0.37450981 -0.48039216 -0.45294118 -0.46470588
  -0.49607843 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5
  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5
  -0.46470588 -0.4254902  -0.48431373 -0.38235295 -0.45686275 -0.46862745
  -0.19803922 -0.44901961]]
</code></pre>
</div>

<p><img src="output_12_1.png" alt="png" /></p>

<p>Merge and prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune <code class="highlighter-rouge">train_size</code> as needed. The labels will be stored into a separate array of integers 0 through 9.</p>

<p>Also create a validation dataset for hyperparameter tuning.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_arrays</span><span class="p">(</span><span class="n">nb_rows</span><span class="p">,</span> <span class="n">img_size</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">nb_rows</span><span class="p">:</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="n">nb_rows</span><span class="p">,</span> <span class="n">img_size</span><span class="p">,</span> <span class="n">img_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">nb_rows</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">dataset</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
  <span class="k">return</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">labels</span>

<span class="k">def</span> <span class="nf">merge_datasets</span><span class="p">(</span><span class="n">pickle_files</span><span class="p">,</span> <span class="n">train_size</span><span class="p">,</span> <span class="n">valid_size</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pickle_files</span><span class="p">)</span>
  <span class="n">valid_dataset</span><span class="p">,</span> <span class="n">valid_labels</span> <span class="o">=</span> <span class="n">make_arrays</span><span class="p">(</span><span class="n">valid_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">)</span>
  <span class="n">train_dataset</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">make_arrays</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">)</span>
  <span class="n">vsize_per_class</span> <span class="o">=</span> <span class="n">valid_size</span> <span class="o">//</span> <span class="n">num_classes</span>
  <span class="n">tsize_per_class</span> <span class="o">=</span> <span class="n">train_size</span> <span class="o">//</span> <span class="n">num_classes</span>

  <span class="n">start_v</span><span class="p">,</span> <span class="n">start_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
  <span class="n">end_v</span><span class="p">,</span> <span class="n">end_t</span> <span class="o">=</span> <span class="n">vsize_per_class</span><span class="p">,</span> <span class="n">tsize_per_class</span>
  <span class="n">end_l</span> <span class="o">=</span> <span class="n">vsize_per_class</span><span class="o">+</span><span class="n">tsize_per_class</span>
  <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">pickle_file</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pickle_files</span><span class="p">):</span>       
    <span class="k">try</span><span class="p">:</span>
      <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">pickle_file</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">letter_set</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="c"># let's shuffle the letters to have random validation and training set</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">letter_set</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">valid_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="n">valid_letter</span> <span class="o">=</span> <span class="n">letter_set</span><span class="p">[:</span><span class="n">vsize_per_class</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
          <span class="n">valid_dataset</span><span class="p">[</span><span class="n">start_v</span><span class="p">:</span><span class="n">end_v</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">valid_letter</span>
          <span class="n">valid_labels</span><span class="p">[</span><span class="n">start_v</span><span class="p">:</span><span class="n">end_v</span><span class="p">]</span> <span class="o">=</span> <span class="n">label</span>
          <span class="n">start_v</span> <span class="o">+=</span> <span class="n">vsize_per_class</span>
          <span class="n">end_v</span> <span class="o">+=</span> <span class="n">vsize_per_class</span>

        <span class="n">train_letter</span> <span class="o">=</span> <span class="n">letter_set</span><span class="p">[</span><span class="n">vsize_per_class</span><span class="p">:</span><span class="n">end_l</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="n">train_dataset</span><span class="p">[</span><span class="n">start_t</span><span class="p">:</span><span class="n">end_t</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">train_letter</span>
        <span class="n">train_labels</span><span class="p">[</span><span class="n">start_t</span><span class="p">:</span><span class="n">end_t</span><span class="p">]</span> <span class="o">=</span> <span class="n">label</span>
        <span class="n">start_t</span> <span class="o">+=</span> <span class="n">tsize_per_class</span>
        <span class="n">end_t</span> <span class="o">+=</span> <span class="n">tsize_per_class</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'Unable to process data from'</span><span class="p">,</span> <span class="n">pickle_file</span><span class="p">,</span> <span class="s">':'</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
      <span class="k">raise</span>

  <span class="k">return</span> <span class="n">valid_dataset</span><span class="p">,</span> <span class="n">valid_labels</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">train_labels</span>


<span class="n">train_size</span> <span class="o">=</span> <span class="mi">200000</span>
<span class="n">valid_size</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">valid_dataset</span><span class="p">,</span> <span class="n">valid_labels</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">merge_datasets</span><span class="p">(</span>
  <span class="n">train_datasets</span><span class="p">,</span> <span class="n">train_size</span><span class="p">,</span> <span class="n">valid_size</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">merge_datasets</span><span class="p">(</span><span class="n">test_datasets</span><span class="p">,</span> <span class="n">test_size</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Training:'</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Validation:'</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">valid_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Testing:'</span><span class="p">,</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Training: (200000, 28, 28) (200000,)
Validation: (10000, 28, 28) (10000,)
Testing: (10000, 28, 28) (10000,)
</code></pre>
</div>

<hr />
<p>Problem 3
———
Another check: we expect the data to be balanced across classes. Verify that.</p>

<hr />

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="n">Counter</span><span class="p">(</span><span class="n">train_labels</span><span class="p">),</span> <span class="n">Counter</span><span class="p">(</span><span class="n">test_labels</span><span class="p">),</span> <span class="n">Counter</span><span class="p">(</span><span class="n">valid_labels</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(Counter({0: 20000,
          1: 20000,
          2: 20000,
          3: 20000,
          4: 20000,
          5: 20000,
          6: 20000,
          7: 20000,
          8: 20000,
          9: 20000}),
 Counter({0: 1000,
          1: 1000,
          2: 1000,
          3: 1000,
          4: 1000,
          5: 1000,
          6: 1000,
          7: 1000,
          8: 1000,
          9: 1000}),
 Counter({0: 1000,
          1: 1000,
          2: 1000,
          3: 1000,
          4: 1000,
          5: 1000,
          6: 1000,
          7: 1000,
          8: 1000,
          9: 1000}))
</code></pre>
</div>

<p>Next, we’ll randomize the data. It’s important to have the labels well shuffled for the training and test distributions to match.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">randomize</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
  <span class="n">permutation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">shuffled_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">permutation</span><span class="p">,:,:]</span>
  <span class="n">shuffled_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">permutation</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">shuffled_dataset</span><span class="p">,</span> <span class="n">shuffled_labels</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">randomize</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">test_dataset</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">randomize</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
<span class="n">valid_dataset</span><span class="p">,</span> <span class="n">valid_labels</span> <span class="o">=</span> <span class="n">randomize</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">valid_labels</span><span class="p">)</span>
</code></pre>
</div>

<hr />
<p>Problem 4
———
Convince yourself that the data is still good after shuffling!</p>

<hr />

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">test_labels</span><span class="o">.</span><span class="n">shape</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(10000,)
</code></pre>
</div>

<p>Finally, let’s save the data for later reuse:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">pickle_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_root</span><span class="p">,</span> <span class="s">'notMNIST.pickle'</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
  <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">pickle_file</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span>
  <span class="n">save</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'train_dataset'</span><span class="p">:</span> <span class="n">train_dataset</span><span class="p">,</span>
    <span class="s">'train_labels'</span><span class="p">:</span> <span class="n">train_labels</span><span class="p">,</span>
    <span class="s">'valid_dataset'</span><span class="p">:</span> <span class="n">valid_dataset</span><span class="p">,</span>
    <span class="s">'valid_labels'</span><span class="p">:</span> <span class="n">valid_labels</span><span class="p">,</span>
    <span class="s">'test_dataset'</span><span class="p">:</span> <span class="n">test_dataset</span><span class="p">,</span>
    <span class="s">'test_labels'</span><span class="p">:</span> <span class="n">test_labels</span><span class="p">,</span>
    <span class="p">}</span>
  <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">save</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>
  <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Unable to save data to'</span><span class="p">,</span> <span class="n">pickle_file</span><span class="p">,</span> <span class="s">':'</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
  <span class="k">raise</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">stat</span><span class="p">(</span><span class="n">pickle_file</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Compressed pickle size:'</span><span class="p">,</span> <span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Compressed pickle size: 690800512
</code></pre>
</div>

<hr />
<p>Problem 5
———</p>

<p>By construction, this dataset might contain a lot of overlapping samples, including training data that’s also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it.
Measure how much overlap there is between training, validation and test samples.</p>

<p>Optional questions:</p>
<ul>
  <li>What about near duplicates between datasets? (images that are almost identical)</li>
  <li>
    <h2 id="create-a-sanitized-validation-and-test-set-and-compare-your-accuracy-on-those-in-subsequent-assignments">Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments.</h2>
  </li>
</ul>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">valid_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">test_dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">test_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">train_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>

</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(200000, 28, 28)
(200000,)
(10000, 28, 28)
(10000,)
(10000, 28, 28)
(10000,)
[4 9 6 2 7 3 5 9 6 4 7 6 0 1 8 0 1 9 6 5]
</code></pre>
</div>

<hr />
<p>Problem 6
———</p>

<p>Let’s get an idea of what an off-the-shelf classifier can give you on this data. It’s always good to check that there is something to learn, and that it’s a problem that is not so trivial that a canned solution solves it.</p>

<p>Train a simple model on this data using 50, 100, 1000 and 5000 training samples. Hint: you can use the LogisticRegression model from sklearn.linear_model.</p>

<p>Optional question: train an off-the-shelf model on all the data!</p>

<hr />

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">datas</span><span class="p">,</span> <span class="n">classes</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span> <span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">datas</span> <span class="p">:</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
        <span class="n">encoded</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">test_labels</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">test_labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>3 [0 0 0 1 0 0 0 0 0 0]
6 [0 0 0 0 0 0 1 0 0 0]
4 [0 0 0 0 1 0 0 0 0 0]
7 [0 0 0 0 0 0 0 1 0 0]
7 [0 0 0 0 0 0 0 1 0 0]
7 [0 0 0 0 0 0 0 1 0 0]
7 [0 0 0 0 0 0 0 1 0 0]
1 [0 1 0 0 0 0 0 0 0 0]
7 [0 0 0 0 0 0 0 1 0 0]
4 [0 0 0 0 1 0 0 0 0 0]
6 [0 0 0 0 0 0 1 0 0 0]
0 [1 0 0 0 0 0 0 0 0 0]
9 [0 0 0 0 0 0 0 0 0 1]
0 [1 0 0 0 0 0 0 0 0 0]
0 [1 0 0 0 0 0 0 0 0 0]
2 [0 0 1 0 0 0 0 0 0 0]
2 [0 0 1 0 0 0 0 0 0 0]
8 [0 0 0 0 0 0 0 0 1 0]
2 [0 0 1 0 0 0 0 0 0 0]
9 [0 0 0 0 0 0 0 0 0 1]
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="p">(</span><span class="mi">200000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">train_labels</span><span class="p">)</span>

<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">valid_labels</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">valid_labels</span><span class="p">)</span>

</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(200000, 784) (200000, 10)
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># HyperParameters</span>
<span class="n">data_len</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="mi">15000</span>

<span class="c"># Hyperparameters for Stochastic Gradient Descent</span>
<span class="n">mini_batch_size</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>



<span class="c"># Feed</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span> <span class="p">,</span> <span class="mi">784</span><span class="p">]</span> <span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span> <span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>



</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Neural Network Model</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"layer_1"</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span> <span class="p">:</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"w1"</span><span class="p">,</span>
                        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
                        <span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">))</span>

    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"b1"</span><span class="p">,</span>
                        <span class="n">shape</span> <span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
                        <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">))</span>


<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"layer_2"</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span> <span class="p">:</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"w2"</span><span class="p">,</span>
                        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                        <span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">))</span>

    <span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"b2"</span><span class="p">,</span>
                        <span class="n">shape</span> <span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
                        <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">))</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c">#operations</span>
<span class="n">layer1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span> <span class="p">,</span> <span class="n">b1</span><span class="p">)</span>  <span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span> <span class="n">b2</span><span class="p">)</span>

<span class="n">prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span> <span class="o">=</span> <span class="n">train_y</span> <span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">)</span>  <span class="c">#tf.reduce_sum(tf.square(prob - train_y))</span>

<span class="n">loss_sum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>

<span class="n">training</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss_sum</span><span class="p">)</span>



<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">loss_log</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">step_log</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span> <span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">)</span> <span class="p">:</span>  <span class="c">#stochastic gradient descent</span>
        <span class="c">#mini_batch 획득</span>
        <span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="p">)</span>
        <span class="n">mini_batch_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
        <span class="n">mini_batch_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>

        <span class="c">#획득한 mini_batch 만을 통해 gradient descent 실행해 W,b parameter update</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">feed_dict</span> <span class="o">=</span><span class="p">{</span><span class="n">train_x</span> <span class="p">:</span> <span class="n">mini_batch_dataset</span><span class="p">,</span> <span class="n">train_y</span> <span class="p">:</span> <span class="n">mini_batch_labels</span> <span class="p">}</span> <span class="p">)</span>
        <span class="n">ls</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">loss_sum</span> <span class="p">,</span> <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">train_x</span> <span class="p">:</span> <span class="n">mini_batch_dataset</span><span class="p">,</span>  <span class="n">train_y</span> <span class="p">:</span> <span class="n">mini_batch_labels</span><span class="p">})</span>

        <span class="n">loss_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ls</span><span class="p">)</span>
        <span class="n">step_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"step :"</span> <span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="s">" loss in current mini_batch : "</span> <span class="p">,</span><span class="n">ls</span><span class="p">)</span>


    <span class="k">print</span><span class="p">(</span><span class="s">"Finish stochastic gradient descent Optimization !"</span><span class="p">)</span>

    <span class="n">prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">train_y</span> <span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="s">"float"</span><span class="p">))</span>

    <span class="n">ac</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span> <span class="p">,</span> <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">train_x</span> <span class="p">:</span> <span class="n">valid_dataset</span> <span class="p">,</span> <span class="n">train_y</span> <span class="p">:</span> <span class="n">valid_labels</span><span class="p">})</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Accuracy for validation set:"</span><span class="p">,</span> <span class="n">ac</span><span class="p">)</span>


</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>step : 0  loss in current mini_batch :  4601.67
step : 100  loss in current mini_batch :  1326.26
step : 200  loss in current mini_batch :  1195.37
step : 300  loss in current mini_batch :  1190.59
step : 400  loss in current mini_batch :  1132.75
step : 500  loss in current mini_batch :  1044.28
step : 600  loss in current mini_batch :  1068.86
step : 700  loss in current mini_batch :  974.242
step : 800  loss in current mini_batch :  1004.78
step : 900  loss in current mini_batch :  965.052
step : 1000  loss in current mini_batch :  1032.54
step : 1100  loss in current mini_batch :  948.49
step : 1200  loss in current mini_batch :  992.098
step : 1300  loss in current mini_batch :  898.133
step : 1400  loss in current mini_batch :  937.777
step : 1500  loss in current mini_batch :  907.827
step : 1600  loss in current mini_batch :  965.427
step : 1700  loss in current mini_batch :  918.821
step : 1800  loss in current mini_batch :  862.558
step : 1900  loss in current mini_batch :  920.642
step : 2000  loss in current mini_batch :  908.376
step : 2100  loss in current mini_batch :  952.623
step : 2200  loss in current mini_batch :  922.263
step : 2300  loss in current mini_batch :  759.279
step : 2400  loss in current mini_batch :  884.643
step : 2500  loss in current mini_batch :  820.881
step : 2600  loss in current mini_batch :  895.333
step : 2700  loss in current mini_batch :  813.88
step : 2800  loss in current mini_batch :  837.536
step : 2900  loss in current mini_batch :  761.909
step : 3000  loss in current mini_batch :  796.58
step : 3100  loss in current mini_batch :  831.549
step : 3200  loss in current mini_batch :  762.656
step : 3300  loss in current mini_batch :  704.09
step : 3400  loss in current mini_batch :  755.157
step : 3500  loss in current mini_batch :  753.041
step : 3600  loss in current mini_batch :  757.668
step : 3700  loss in current mini_batch :  717.796
step : 3800  loss in current mini_batch :  680.764
step : 3900  loss in current mini_batch :  722.954
step : 4000  loss in current mini_batch :  729.337
step : 4100  loss in current mini_batch :  716.981
step : 4200  loss in current mini_batch :  776.64
step : 4300  loss in current mini_batch :  704.61
step : 4400  loss in current mini_batch :  717.521
step : 4500  loss in current mini_batch :  747.119
step : 4600  loss in current mini_batch :  720.399
step : 4700  loss in current mini_batch :  723.838
step : 4800  loss in current mini_batch :  728.95
step : 4900  loss in current mini_batch :  768.013
step : 5000  loss in current mini_batch :  762.636
step : 5100  loss in current mini_batch :  674.274
step : 5200  loss in current mini_batch :  725.508
step : 5300  loss in current mini_batch :  644.987
step : 5400  loss in current mini_batch :  700.892
step : 5500  loss in current mini_batch :  712.414
step : 5600  loss in current mini_batch :  686.918
step : 5700  loss in current mini_batch :  729.377
step : 5800  loss in current mini_batch :  659.036
step : 5900  loss in current mini_batch :  717.935
step : 6000  loss in current mini_batch :  717.452
step : 6100  loss in current mini_batch :  751.168
step : 6200  loss in current mini_batch :  624.755
step : 6300  loss in current mini_batch :  671.416
step : 6400  loss in current mini_batch :  714.373
step : 6500  loss in current mini_batch :  692.471
step : 6600  loss in current mini_batch :  657.005
step : 6700  loss in current mini_batch :  664.928
step : 6800  loss in current mini_batch :  748.706
step : 6900  loss in current mini_batch :  708.959
step : 7000  loss in current mini_batch :  689.302
step : 7100  loss in current mini_batch :  576.379
step : 7200  loss in current mini_batch :  633.845
step : 7300  loss in current mini_batch :  634.733
step : 7400  loss in current mini_batch :  633.668
step : 7500  loss in current mini_batch :  651.758
step : 7600  loss in current mini_batch :  682.321
step : 7700  loss in current mini_batch :  693.369
step : 7800  loss in current mini_batch :  692.234
step : 7900  loss in current mini_batch :  669.476
step : 8000  loss in current mini_batch :  682.369
step : 8100  loss in current mini_batch :  625.788
step : 8200  loss in current mini_batch :  677.573
step : 8300  loss in current mini_batch :  657.304
step : 8400  loss in current mini_batch :  661.556
step : 8500  loss in current mini_batch :  651.978
step : 8600  loss in current mini_batch :  605.606
step : 8700  loss in current mini_batch :  595.129
step : 8800  loss in current mini_batch :  605.027
step : 8900  loss in current mini_batch :  632.582
step : 9000  loss in current mini_batch :  688.383
step : 9100  loss in current mini_batch :  635.524
step : 9200  loss in current mini_batch :  660.438
step : 9300  loss in current mini_batch :  593.719
step : 9400  loss in current mini_batch :  655.906
step : 9500  loss in current mini_batch :  641.276
step : 9600  loss in current mini_batch :  620.561
step : 9700  loss in current mini_batch :  639.464
step : 9800  loss in current mini_batch :  713.408
step : 9900  loss in current mini_batch :  635.882
step : 10000  loss in current mini_batch :  616.373
step : 10100  loss in current mini_batch :  635.377
step : 10200  loss in current mini_batch :  617.724
step : 10300  loss in current mini_batch :  651.883
step : 10400  loss in current mini_batch :  624.633
step : 10500  loss in current mini_batch :  689.182
step : 10600  loss in current mini_batch :  643.464
step : 10700  loss in current mini_batch :  632.735
step : 10800  loss in current mini_batch :  657.008
step : 10900  loss in current mini_batch :  727.608
step : 11000  loss in current mini_batch :  604.399
step : 11100  loss in current mini_batch :  678.079
step : 11200  loss in current mini_batch :  602.736
step : 11300  loss in current mini_batch :  628.098
step : 11400  loss in current mini_batch :  631.935
step : 11500  loss in current mini_batch :  486.739
step : 11600  loss in current mini_batch :  600.978
step : 11700  loss in current mini_batch :  661.016
step : 11800  loss in current mini_batch :  658.212
step : 11900  loss in current mini_batch :  591.686
step : 12000  loss in current mini_batch :  561.555
step : 12100  loss in current mini_batch :  590.612
step : 12200  loss in current mini_batch :  625.416
step : 12300  loss in current mini_batch :  631.534
step : 12400  loss in current mini_batch :  576.341
step : 12500  loss in current mini_batch :  561.747
step : 12600  loss in current mini_batch :  610.397
step : 12700  loss in current mini_batch :  646.197
step : 12800  loss in current mini_batch :  571.696
step : 12900  loss in current mini_batch :  597.438
step : 13000  loss in current mini_batch :  571.774
step : 13100  loss in current mini_batch :  605.854
step : 13200  loss in current mini_batch :  587.422
step : 13300  loss in current mini_batch :  629.188
step : 13400  loss in current mini_batch :  580.812
step : 13500  loss in current mini_batch :  618.708
step : 13600  loss in current mini_batch :  601.535
step : 13700  loss in current mini_batch :  592.996
step : 13800  loss in current mini_batch :  513.31
step : 13900  loss in current mini_batch :  573.671
step : 14000  loss in current mini_batch :  568.322
step : 14100  loss in current mini_batch :  534.002
step : 14200  loss in current mini_batch :  530.757
step : 14300  loss in current mini_batch :  596.364
step : 14400  loss in current mini_batch :  617.588
step : 14500  loss in current mini_batch :  612.434
step : 14600  loss in current mini_batch :  544.609
step : 14700  loss in current mini_batch :  573.086
step : 14800  loss in current mini_batch :  658.637
step : 14900  loss in current mini_batch :  682.304
Finish stochastic gradient descent Optimization !
Accuracy for validation set: 0.8785
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c">#show loss changes</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">step_log</span><span class="p">[:</span><span class="mi">10000</span><span class="p">],</span> <span class="n">loss_log</span><span class="p">[:</span><span class="mi">10000</span><span class="p">],</span> <span class="s">"r"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="output_36_0.png" alt="png" /></p>

<p>#Run the model</p>

<p>with tf.Session() as sess  :
    sess.run(init)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>for step in range(3000) :

    sess.run(training ,feed_dict = {train_x : train_dataset[:data_len], train_y : train_labels[:data_len]})
    ls = sess.run(loss_sum ,feed_dict = {train_x : train_dataset[:data_len], train_y : train_labels[:data_len]})

    if step % 100 == 0 :
        print("step:", step, "loss:" ,ls)

prediction = tf.equal(tf.argmax(prob ,1), tf.argmax(train_y, 1))
accuracy = tf.reduce_mean(tf.cast(prediction, "float"))
ac = sess.run(accuracy, feed_dict = {train_x : valid_dataset, train_y : valid_labels})

print("\n\n accuracy : " , ac)
</code></pre>
</div>
</div>

    <div class ="container mt-5 mb-5">
  <hr  />
  <div style="display:flex; width: 100%; flex-direction: row; justify-content:center;">
    <img src="/asset/media/image/logo.jpg" width="50" height="50" class="border border-primary px-auto" alt="" style="border-radius:10px;">
    
  </div>
</div>


    <!--bootstrap Javascript-->
     <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh" crossorigin="anonymous"></script>
     <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ" crossorigin="anonymous"></script>
     <!--Custom Javascript-->
     <script src="/asset/static/post_load.js"></script>
     <script src="/asset/static/post_table_generation.js"></script>
     <script>
  $(document).ready(function() {
    var main_route = (window.location.pathname.split("/")[1]);
    $('#navbar_' + main_route).addClass('active');
    navbar = $('#navbarbackground');
    logotext = $('#logo_text');
    if (main_route == "post"){
      // navbar.attr('style',"background-color:rgb(146, 146, 146); background-image:url('/asset/media/image/gradient4.png');   background-blend-mode: color; background-size: cover;");
      logotext.text('post');
      logotext.attr('style',"color:#213b80;");
    }
    else if(main_route == "project"){
      // navbar.attr('style',"background-color:rgba(157, 157, 157, 0.54); background-image:url('/asset/media/image/gradient3.jpg');background-blend-mode:color; background-size:cover;");
      logotext.text('project');
      logotext.attr('style',"color:#6f1c16;");
    }
    else if(main_route == "profile"){
      // navbar.attr('style',"background-color:rgba(190, 190, 190, 0.75); background-image:url('/asset/media/image/gradient2-1.jpg'); background-blend-mode: color; background-size: cover;");
      logotext.text('profile');
      logotext.attr('style',"color:#6849af;");
    }
    else{
      // navbar.attr('style',"background-color : rgba(178, 85, 228, 0.94); background-image:url('/asset/media/image/gradient1.jpg');   background-blend-mode: color; background-size: cover;");
    }
  });
</script>

     <script>
  $(document).ready(function() {
    var main_route = (window.location.pathname.split("/")[2]);
    $('#categorybar_' + main_route).addClass('active').addClass('bg-dark');
  });
</script>

  </body>
</html>
