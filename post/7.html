<!doctype html>
<html>
  <head>

    <title>SOM: Self Organazing Map 으로 Clustering 코드구현 까지 | Jayne.who();</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!--bootstrap CSS-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb" crossorigin="anonymous">
    <!--code highlighter CSS for Jekyll Markdown to HTML Converter -->
    <link rel="stylesheet" href="/asset/static/pygments-codehighlight-css/monokai.css" />
    <link rel="stylesheet" href="/asset/static/post_load.css" />
    <link rel="stylesheet" href="/asset/static/font/stylesheets/NotoSansKR-Hestia.css" />
    <style>
      body {font-family: 'Noto Sans Korean', sans-serif;font-weight: 350;padding-top: 50px;}
      h1,h2,h3,h4,h5,h6 {font-weight : 800;color : #148b8e;}
      .navbar, .nav {font-weight: 800;}
      a {color : #90b3d8;}
    </style>
    <!--Tawk.to Script-->
    <script src="/asset/static/tawk-chat-api.js"></script>
  </head>
  <body>

    

<nav id = "navbarbackground" class="navbar fixed-top navbar-expand-lg navbar-light" style="background-color : rgba(178, 85, 228, 0.94); background-image:url('/asset/media/image/gradient1.jpg');   background-blend-mode: color;
  background-size: cover;">
  <a class="navbar-brand" href="/">
    <img src="/asset/media/image/logo.jpg" width="35" height="35" class="d-inline-block align-top border border-primary" alt="" style="border-radius:10px">
    Jayne.who(<p id="logo_text" class="d-inline"></p>);
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav" style="font-weight : 600;">
      <li class="nav-item" id="navbar_profile">
        <a class="nav-link" href="/profile/">Profile </a>
      </li>
      <li class="nav-item" id="navbar_post">
        <a class="nav-link" href="/post/">Posts </a>
      </li>
      <li class="nav-item" id="navbar_project">
        <a class="nav-link" href="/project/">Projects </a>
      </li>
    </ul>
  </div>
</nav>

    <style>
/*post titles style*/
.post h1,h2,h3,h4,h5,h6 {
  color: #3f71f4;

}
.post h1 {font-size: 2.0rem;padding-top: 4.5rem;padding-bottom:1rem;}
.post h2 {font-size: 1.7rem;}
.post h3 {font-size: 1.6rem;}
.post h4 {font-size: 1.4rem;}
.post h5 {font-size: 1.2rem;}
/*for image*/
.post img {
  max-width: 100%;
  height: auto;
  padding-top : 2rem ;
  padding-bottom : 2rem;
}

/*for code highlighter*/
pre {
  background-color: #494b4c ;
  border-radius: 6px;
  padding: 10px;
  color : #d0d7de;
}


</style>

<div class="container-fluid text-center pt-3" style="background-image : url('/asset/media/image/post/7/4.png');  background-color: #00010299; background-blend-mode: color; background-size: cover; min-height: 350px;">
  <a href="/post/"><p class="text-left md-5"><button type="button" class="btn btn-outline-secondary btn-sm d-inline text-uppercase"> < Back </button></p></a>
  <h1 class="card-title text-white" style="font-size : 2.2rem;">SOM: Self Organazing Map 으로 Clustering 코드구현 까지</h1>
  <p class="card-text text-white"><p class="text-muted">deeplearning | 27 July 2017</p></p>
  <p class="card-text text-white">
    Tags |
    
    <a href="#" class="badge badge-primary">sklearn</a>
    
    <a href="#" class="badge badge-primary">python</a>
    
  </p>
</div>


<div class="post container pt-5 " style = "max-width : 750px"><p>Self Organizing Map 에 대해서 알아보겠습니다.</p>

<p>Udemy 의 <a href="https://www.udemy.com/deeplearning/learn/v4/overview">Deep-Learning-A-to-Z 강의</a> 의 SOM 파트를 수강하고 작성하였습니다.</p>

<p>Self Organizing Map은 줄여서 SOM 이라고 부릅니다.</p>

<p>Unsupervised learning 방법 중 하나이며 Clustering 에 쓰입니다.</p>

<p>Clustering 작업을 수행하는, SOM 보다 조금 단순한 K-mean 알고리즘을 보고 SOM 을 보면 이해가 쉽습니다.</p>

<h1 id="k-mean-cluster">K-mean Cluster</h1>

<h2 id="k-mean-의-결과--clustering">K-mean 의 결과 = Clustering</h2>

<p>아래의 그림은 K-mean 알고리즘으로 클러스터링 한 결과를 보여줍니다.
<img src="/asset/media/image/post/7/1.png" alt="image.png" /></p>

<h2 id="k-mean-의-학습-과정">K-mean 의 학습 과정</h2>

<p><img src="/asset/media/image/post/7/2.png" alt="image.png" /></p>

<p>K-mean 의 학습과정은 단순합니다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: left">과정</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">step1</td>
      <td style="text-align: left">몇개의 군집으로 나눌것인지 K 를 설정해줍니다.</td>
    </tr>
    <tr>
      <td style="text-align: center">step2</td>
      <td style="text-align: left">군집으로 나눌 데이터 분포들 사이에 K개의 랜덤한 <strong>센트로이드 포인트</strong>들을 잡습니다.(랜덤하게 잡으면 엉뚱하게 Clustering하는 에러도 있기도 합니다)</td>
    </tr>
    <tr>
      <td style="text-align: center">step3</td>
      <td style="text-align: left">각각의 데이터 포인트틀과 가장 가까운 센트로이드 포인트를 찾습니다. (이것이 K Cluser 형태가 됩니다)</td>
    </tr>
    <tr>
      <td style="text-align: center">step4</td>
      <td style="text-align: left">K 개로 나뉜 각 군집의 새로운 중심점을 찾아 새로운 센트로이드 포인트로 지정합니다.</td>
    </tr>
    <tr>
      <td style="text-align: center">step5</td>
      <td style="text-align: left">새로운 센트로이드 포인트에 대해 다시 step3 처럼 가까운 데이터포인트들을 묶습니다.(K Cluster 형태가 됩니다)</td>
    </tr>
    <tr>
      <td style="text-align: center">…</td>
      <td style="text-align: left">센트로이드 포인트 이동이 멈추기 전까지 step4 로 다시 돌아갑니다.</td>
    </tr>
  </tbody>
</table>

<h1 id="som-structrue">SOM Structrue</h1>

<p>SOM 은 조금 다릅니다.</p>

<p>SOM 은 우선 Map size 를 설정해줍니다. (2차원으로 x=3, y=3 크기의 Map 이라고 가정합시다)</p>

<p>Map Size 를 설정해, 전혀 학습하지 않은 생 Map 을 Default Map 이라고 칭하겠습니다.</p>

<p>이제 이 Default Map 을 조금씩 수정해서(학습해서) 데이터 분포 형태에 Map 을 근사시키려고 합니다.</p>

<p>3*3 사이즈의 map 은 9개의 map point = Node 로 이루어졍있고, 각 Node 는 데이터의 차원수와 동일한 parameter 갯수를 가집니다.</p>

<p>사진에서는 3차원 X(x1,x2,x3) 데이터 이므로  Node_i(w_i1, w_i2, w_i3) 가 i=1~9 로 존재합니다.</p>

<p>이제 이 각각의 Node 들과 각 데이터 X 사이의 거리를 구합니다.</p>

<p>이들중 데이터와 가장 가까운 node 를 winning node 라고 마킹합니다.</p>

<p><img src="/asset/media/image/post/7/3.png" alt="image.png" /></p>

<p>점(w1i,w2i,w3i)들과 데이터의 feature인 (x1,x2,x3) 점과의 distance 를 좁혀나갑니다.</p>

<p>아래는 map 을 데이터에 맞게 점점 fitting 시키는 모습입니다.</p>

<p>특정 데이터 점 하나가 SOMap 의 한 점을 점점 끌어당기는 모습입니다.</p>

<p>(실제론 winning node 와 그 주변 node 까지 동시에 끌어당깁니다.)</p>

<p>이런식으로 모든 데이터에 대해 SOMap 의 각 점을 끌어오면 SOMap 완성됩니다.</p>

<p><img src="/asset/media/image/post/7/4.png" alt="image.png" /></p>

<h1 id="som의-특징">SOM의 특징</h1>

<p>SOM 의 특징은 아래와 같습니다.</p>

<ul>
  <li>SOM 은 Input 데이터들 사이의 위상을 잘 나타냅니다.</li>
  <li>SOM 은 잘 구별되지 않는 데이터간의 correlation 을 찾아낼 수 있습니다.</li>
  <li>SOM 은 비지도 학습으로 Clustering 을 수행할 수 있습니다.</li>
  <li>비지도이고, Label Data 가 없으므로, Back propagation 과정도 없습니다.</li>
  <li>output node 간에 후속 연결이 없습니다.</li>
</ul>

<p><img src="/asset/media/image/post/7/5.png" alt="image.png" /></p>

<h1 id="training-전략">Training 전략</h1>

<p>자세한  SOM 의 학습 과정은 아래와 같습니다.</p>

<p><img src="/asset/media/image/post/7/6.png" alt="image.png" /></p>

<h1 id="python-code">Python Code</h1>
<ol>
  <li>SOM model code</li>
  <li>SOM model use code</li>
</ol>

<h2 id="1-som-model-code">1. SOM model code</h2>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>

<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">unravel_index</span><span class="p">,</span> <span class="n">nditer</span><span class="p">,</span> <span class="n">linalg</span><span class="p">,</span> <span class="n">random</span><span class="p">,</span> <span class="n">subtract</span><span class="p">,</span>
                   <span class="n">power</span><span class="p">,</span> <span class="n">exp</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">zeros</span><span class="p">,</span> <span class="n">arange</span><span class="p">,</span> <span class="n">outer</span><span class="p">,</span> <span class="n">meshgrid</span><span class="p">,</span> <span class="n">dot</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">warn</span>


<span class="s">"""
    Minimalistic implementation of the Self Organizing Maps (SOM).
"""</span>


<span class="k">def</span> <span class="nf">fast_norm</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="s">"""Returns norm-2 of a 1-D numpy array.

    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).
    """</span>
    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">MiniSom</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">input_len</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">decay_function</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="s">"""
            Initializes a Self Organizing Maps.

            x,y - dimensions of the SOM

            input_len - number of the elements of the vectors in input

            sigma - spread of the neighborhood function (Gaussian), needs to be adequate to the dimensions of the map.
            (at the iteration t we have sigma(t) = sigma / (1 + t/T) where T is #num_iteration/2)

            learning_rate - initial learning rate
            (at the iteration t we have learning_rate(t) = learning_rate / (1 + t/T) where T is #num_iteration/2)

            decay_function, function that reduces learning_rate and sigma at each iteration
                            default function: lambda x,current_iteration,max_iter: x/(1+current_iteration/max_iter)

            random_seed, random seed to use.
        """</span>
        <span class="k">if</span> <span class="n">sigma</span> <span class="o">&gt;=</span> <span class="n">x</span><span class="o">/</span><span class="mf">2.0</span> <span class="ow">or</span> <span class="n">sigma</span> <span class="o">&gt;=</span> <span class="n">y</span><span class="o">/</span><span class="mf">2.0</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span><span class="s">'Warning: sigma is too high for the dimension of the map.'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">random_seed</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">decay_function</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_decay_function</span> <span class="o">=</span> <span class="n">decay_function</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_decay_function</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">:</span> <span class="n">x</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">t</span><span class="o">/</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">input_len</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="o">-</span><span class="mi">1</span> <span class="c"># random initialization</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">fast_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span> <span class="c"># normalization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_map</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neigx</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neigy</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c"># used to evaluate the neighborhood function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neighborhood</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gaussian</span>

    <span class="k">def</span> <span class="nf">_activate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">""" Updates matrix activation_map, in this matrix the element i,j is the response of the neuron i,j to x """</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">subtract</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="c"># x - w</span>
        <span class="n">it</span> <span class="o">=</span> <span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_map</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s">'multi_index'</span><span class="p">])</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation_map</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">fast_norm</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">])</span>  <span class="c"># || x - w ||</span>
            <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">activate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">""" Returns the activation map to x """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_map</span>

    <span class="k">def</span> <span class="nf">gaussian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="s">""" Returns a Gaussian centered in c """</span>
        <span class="n">d</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">*</span><span class="n">sigma</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">power</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neigx</span><span class="o">-</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">d</span><span class="p">)</span>
        <span class="n">ay</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">power</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neigy</span><span class="o">-</span><span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">d</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outer</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">)</span>  <span class="c"># the external product gives a matrix</span>

    <span class="k">def</span> <span class="nf">diff_gaussian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="s">""" Mexican hat centered in c (unused) """</span>
        <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">meshgrid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neigx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">neigy</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">power</span><span class="p">(</span><span class="n">xx</span><span class="o">-</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">power</span><span class="p">(</span><span class="n">yy</span><span class="o">-</span><span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">*</span><span class="n">sigma</span>
        <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">p</span><span class="o">/</span><span class="n">d</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">/</span><span class="n">d</span><span class="o">*</span><span class="n">p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">winner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">""" Computes the coordinates of the winning neuron for the sample x """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">unravel_index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_map</span><span class="o">.</span><span class="n">argmin</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_map</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">win</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="s">"""
            Updates the weights of the neurons.
            x - current pattern to learn
            win - position of the winning neuron for x (array or tuple).
            t - iteration index
        """</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decay_function</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">sig</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decay_function</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c"># sigma and learning rate decrease with the same rule</span>
        <span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neighborhood</span><span class="p">(</span><span class="n">win</span><span class="p">,</span> <span class="n">sig</span><span class="p">)</span><span class="o">*</span><span class="n">eta</span> <span class="c"># improves the performances</span>
        <span class="n">it</span> <span class="o">=</span> <span class="n">nditer</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s">'multi_index'</span><span class="p">])</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="c"># eta * neighborhood_function * (x-w)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">]</span> <span class="o">+=</span> <span class="n">g</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">])</span>
            <span class="c"># normalization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">]</span> <span class="o">/</span> <span class="n">fast_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">])</span>
            <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">quantization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="s">""" Assigns a code book (weights vector of the winning neuron) to each sample in data. """</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="n">q</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">winner</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">q</span>

    <span class="k">def</span> <span class="nf">random_weights_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="s">""" Initializes the weights of the SOM picking random samples from data """</span>
        <span class="n">it</span> <span class="o">=</span> <span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_map</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s">'multi_index'</span><span class="p">])</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">]</span><span class="o">/</span><span class="n">fast_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">])</span>
            <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">train_random</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">num_iteration</span><span class="p">):</span>
        <span class="s">""" Trains the SOM picking samples at random from data """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_T</span><span class="p">(</span><span class="n">num_iteration</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iteration</span><span class="p">):</span>
            <span class="n">rand_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span> <span class="c"># pick a random sample</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">rand_i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">winner</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">rand_i</span><span class="p">]),</span> <span class="n">iteration</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">num_iteration</span><span class="p">):</span>
        <span class="s">""" Trains using all the vectors in data sequentially """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_T</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="n">num_iteration</span><span class="p">)</span>
        <span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">iteration</span> <span class="o">&lt;</span> <span class="n">num_iteration</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">iteration</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">winner</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]),</span> <span class="n">iteration</span><span class="p">)</span>
            <span class="n">iteration</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_init_T</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_iteration</span><span class="p">):</span>
        <span class="s">""" Initializes the parameter T needed to adjust the learning rate """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">num_iteration</span><span class="o">/</span><span class="mi">2</span>  <span class="c"># keeps the learning rate nearly constant for the last half of the iterations</span>

    <span class="k">def</span> <span class="nf">distance_map</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">""" Returns the distance map of the weights.
            Each cell is the normalised sum of the distances between a neuron and its neighbours.
        """</span>
        <span class="n">um</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">it</span> <span class="o">=</span> <span class="n">nditer</span><span class="p">(</span><span class="n">um</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s">'multi_index'</span><span class="p">])</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">2</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">jj</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">2</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">ii</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ii</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">jj</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">jj</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                        <span class="n">um</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">]</span> <span class="o">+=</span> <span class="n">fast_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">ii</span><span class="p">,</span> <span class="n">jj</span><span class="p">,</span> <span class="p">:]</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">it</span><span class="o">.</span><span class="n">multi_index</span><span class="p">])</span>
            <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span>
        <span class="n">um</span> <span class="o">=</span> <span class="n">um</span><span class="o">/</span><span class="n">um</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">um</span>

    <span class="k">def</span> <span class="nf">activation_response</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="s">"""
            Returns a matrix where the element i,j is the number of times
            that the neuron i,j have been winner.
        """</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">a</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">winner</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">quantization_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="s">"""
            Returns the quantization error computed as the average distance between
            each input sample and its best matching unit.
        """</span>
        <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">error</span> <span class="o">+=</span> <span class="n">fast_norm</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">winner</span><span class="p">(</span><span class="n">x</span><span class="p">)])</span>
        <span class="k">return</span> <span class="n">error</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">win_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="s">"""
            Returns a dictionary wm where wm[(i,j)] is a list with all the patterns
            that have been mapped in the position i,j.
        """</span>
        <span class="n">winmap</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">winmap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">winner</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">winmap</span>

<span class="c">### unit tests</span>
<span class="s">'''
from numpy.testing import assert_almost_equal, assert_array_almost_equal, assert_array_equal


class TestMinisom:
    def setup_method(self, method):
        self.som = MiniSom(5, 5, 1)
        for i in range(5):
            for j in range(5):
                assert_almost_equal(1.0, linalg.norm(self.som.weights[i,j]))  # checking weights normalization
        self.som.weights = zeros((5, 5))  # fake weights
        self.som.weights[2, 3] = 5.0
        self.som.weights[1, 1] = 2.0

    def test_decay_function(self):
        assert self.som._decay_function(1., 2., 3.) == 1./(1.+2./3.)

    def test_fast_norm(self):
        assert fast_norm(array([1, 3])) == sqrt(1+9)

    def test_gaussian(self):
        bell = self.som.gaussian((2, 2), 1)
        assert bell.max() == 1.0
        assert bell.argmax() == 12  # unravel(12) = (2,2)

    def test_win_map(self):
        winners = self.som.win_map([5.0, 2.0])
        assert winners[(2, 3)][0] == 5.0
        assert winners[(1, 1)][0] == 2.0

    def test_activation_reponse(self):
        response = self.som.activation_response([5.0, 2.0])
        assert response[2, 3] == 1
        assert response[1, 1] == 1

    def test_activate(self):
        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)

    def test_quantization_error(self):
        self.som.quantization_error([5, 2]) == 0.0
        self.som.quantization_error([4, 1]) == 0.5

    def test_quantization(self):
        q = self.som.quantization(array([4, 2]))
        assert q[0] == 5.0
        assert q[1] == 2.0

    def test_random_seed(self):
        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)
        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)
        assert_array_almost_equal(som1.weights, som2.weights)  # same initialization
        data = random.rand(100,2)
        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)
        som1.train_random(data,10)
        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)
        som2.train_random(data,10)
        assert_array_almost_equal(som1.weights,som2.weights)  # same state after training

    def test_train_batch(self):
        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)
        data = array([[4, 2], [3, 1]])
        q1 = som.quantization_error(data)
        som.train_batch(data, 10)
        assert q1 &gt; som.quantization_error(data)

    def test_train_random(self):
        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)
        data = array([[4, 2], [3, 1]])
        q1 = som.quantization_error(data)
        som.train_random(data, 10)
        assert q1 &gt; som.quantization_error(data)

    def test_random_weights_init(self):
        som = MiniSom(2, 2, 2, random_seed=1)
        som.random_weights_init(array([[1.0, .0]]))
        for w in som.weights:
            assert_array_equal(w[0], array([1.0, .0]))



'''</span>
</code></pre>
</div>

<h2 id="2-som-model-use-code">2. SOM model use code</h2>

<p>만든 SOM model 을 이용해 Credit card 발급 신청자 데이터를 Clustering 해보고, 수상한 사람을 찾아내보도록 하겠습니다.</p>

<p>우선 데이터를 불러옵니다.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c">#Importing the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'Credit_Card_Applications.csv'</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">dataset</span>
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CustomerID</th>
      <th>A1</th>
      <th>A2</th>
      <th>A3</th>
      <th>A4</th>
      <th>A5</th>
      <th>A6</th>
      <th>A7</th>
      <th>A8</th>
      <th>A9</th>
      <th>A10</th>
      <th>A11</th>
      <th>A12</th>
      <th>A13</th>
      <th>A14</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>15776156</td>
      <td>1</td>
      <td>22.08</td>
      <td>11.460</td>
      <td>2</td>
      <td>4</td>
      <td>4</td>
      <td>1.585</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>100</td>
      <td>1213</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15739548</td>
      <td>0</td>
      <td>22.67</td>
      <td>7.000</td>
      <td>2</td>
      <td>8</td>
      <td>4</td>
      <td>0.165</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>160</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>15662854</td>
      <td>0</td>
      <td>29.58</td>
      <td>1.750</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>1.250</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>280</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15687688</td>
      <td>0</td>
      <td>21.67</td>
      <td>11.500</td>
      <td>1</td>
      <td>5</td>
      <td>3</td>
      <td>0.000</td>
      <td>1</td>
      <td>1</td>
      <td>11</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>15715750</td>
      <td>1</td>
      <td>20.17</td>
      <td>8.170</td>
      <td>2</td>
      <td>6</td>
      <td>4</td>
      <td>1.960</td>
      <td>1</td>
      <td>1</td>
      <td>14</td>
      <td>0</td>
      <td>2</td>
      <td>60</td>
      <td>159</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>15571121</td>
      <td>0</td>
      <td>15.83</td>
      <td>0.585</td>
      <td>2</td>
      <td>8</td>
      <td>8</td>
      <td>1.500</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>100</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>15726466</td>
      <td>1</td>
      <td>17.42</td>
      <td>6.500</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>0.125</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>60</td>
      <td>101</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>15660390</td>
      <td>0</td>
      <td>58.67</td>
      <td>4.460</td>
      <td>2</td>
      <td>11</td>
      <td>8</td>
      <td>3.040</td>
      <td>1</td>
      <td>1</td>
      <td>6</td>
      <td>0</td>
      <td>2</td>
      <td>43</td>
      <td>561</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>15663942</td>
      <td>1</td>
      <td>27.83</td>
      <td>1.000</td>
      <td>1</td>
      <td>2</td>
      <td>8</td>
      <td>3.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>176</td>
      <td>538</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>15638610</td>
      <td>0</td>
      <td>55.75</td>
      <td>7.080</td>
      <td>2</td>
      <td>4</td>
      <td>8</td>
      <td>6.750</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>100</td>
      <td>51</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>15644446</td>
      <td>1</td>
      <td>33.50</td>
      <td>1.750</td>
      <td>2</td>
      <td>14</td>
      <td>8</td>
      <td>4.500</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>2</td>
      <td>253</td>
      <td>858</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>15585892</td>
      <td>1</td>
      <td>41.42</td>
      <td>5.000</td>
      <td>2</td>
      <td>11</td>
      <td>8</td>
      <td>5.000</td>
      <td>1</td>
      <td>1</td>
      <td>6</td>
      <td>1</td>
      <td>2</td>
      <td>470</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>15609356</td>
      <td>1</td>
      <td>20.67</td>
      <td>1.250</td>
      <td>1</td>
      <td>8</td>
      <td>8</td>
      <td>1.375</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>140</td>
      <td>211</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>15803378</td>
      <td>1</td>
      <td>34.92</td>
      <td>5.000</td>
      <td>2</td>
      <td>14</td>
      <td>8</td>
      <td>7.500</td>
      <td>1</td>
      <td>1</td>
      <td>6</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>1001</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15599440</td>
      <td>1</td>
      <td>58.58</td>
      <td>2.710</td>
      <td>2</td>
      <td>8</td>
      <td>4</td>
      <td>2.415</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>320</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>15692408</td>
      <td>1</td>
      <td>48.08</td>
      <td>6.040</td>
      <td>2</td>
      <td>4</td>
      <td>4</td>
      <td>0.040</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>2691</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>15683168</td>
      <td>1</td>
      <td>29.58</td>
      <td>4.500</td>
      <td>2</td>
      <td>9</td>
      <td>4</td>
      <td>7.500</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>330</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>17</th>
      <td>15790254</td>
      <td>0</td>
      <td>18.92</td>
      <td>9.000</td>
      <td>2</td>
      <td>6</td>
      <td>4</td>
      <td>0.750</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>88</td>
      <td>592</td>
      <td>1</td>
    </tr>
    <tr>
      <th>18</th>
      <td>15767729</td>
      <td>1</td>
      <td>20.00</td>
      <td>1.250</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>0.125</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>140</td>
      <td>5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>15768600</td>
      <td>0</td>
      <td>22.42</td>
      <td>5.665</td>
      <td>2</td>
      <td>11</td>
      <td>4</td>
      <td>2.585</td>
      <td>1</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>2</td>
      <td>129</td>
      <td>3258</td>
      <td>1</td>
    </tr>
    <tr>
      <th>20</th>
      <td>15699839</td>
      <td>0</td>
      <td>28.17</td>
      <td>0.585</td>
      <td>2</td>
      <td>6</td>
      <td>4</td>
      <td>0.040</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>260</td>
      <td>1005</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>15786237</td>
      <td>0</td>
      <td>19.17</td>
      <td>0.585</td>
      <td>1</td>
      <td>6</td>
      <td>4</td>
      <td>0.585</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>160</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>15694530</td>
      <td>1</td>
      <td>41.17</td>
      <td>1.335</td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>0.165</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>168</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>15796813</td>
      <td>1</td>
      <td>41.58</td>
      <td>1.750</td>
      <td>2</td>
      <td>4</td>
      <td>4</td>
      <td>0.210</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>160</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>15605791</td>
      <td>1</td>
      <td>19.50</td>
      <td>9.585</td>
      <td>2</td>
      <td>6</td>
      <td>4</td>
      <td>0.790</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>80</td>
      <td>351</td>
      <td>0</td>
    </tr>
    <tr>
      <th>25</th>
      <td>15714087</td>
      <td>1</td>
      <td>32.75</td>
      <td>1.500</td>
      <td>2</td>
      <td>13</td>
      <td>8</td>
      <td>5.500</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>26</th>
      <td>15711446</td>
      <td>1</td>
      <td>22.50</td>
      <td>0.125</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>0.125</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>200</td>
      <td>71</td>
      <td>0</td>
    </tr>
    <tr>
      <th>27</th>
      <td>15588123</td>
      <td>1</td>
      <td>33.17</td>
      <td>3.040</td>
      <td>1</td>
      <td>8</td>
      <td>8</td>
      <td>2.040</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>180</td>
      <td>18028</td>
      <td>1</td>
    </tr>
    <tr>
      <th>28</th>
      <td>15748552</td>
      <td>0</td>
      <td>30.67</td>
      <td>12.000</td>
      <td>2</td>
      <td>8</td>
      <td>4</td>
      <td>2.000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>220</td>
      <td>20</td>
      <td>1</td>
    </tr>
    <tr>
      <th>29</th>
      <td>15618410</td>
      <td>1</td>
      <td>23.08</td>
      <td>2.500</td>
      <td>2</td>
      <td>8</td>
      <td>4</td>
      <td>1.085</td>
      <td>1</td>
      <td>1</td>
      <td>11</td>
      <td>1</td>
      <td>2</td>
      <td>60</td>
      <td>2185</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>660</th>
      <td>15598586</td>
      <td>1</td>
      <td>26.67</td>
      <td>2.710</td>
      <td>1</td>
      <td>13</td>
      <td>4</td>
      <td>5.250</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>211</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>661</th>
      <td>15665014</td>
      <td>0</td>
      <td>22.50</td>
      <td>0.415</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>0.335</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>144</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>662</th>
      <td>15701738</td>
      <td>1</td>
      <td>39.92</td>
      <td>0.540</td>
      <td>1</td>
      <td>6</td>
      <td>4</td>
      <td>0.500</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>2</td>
      <td>200</td>
      <td>1001</td>
      <td>1</td>
    </tr>
    <tr>
      <th>663</th>
      <td>15650591</td>
      <td>0</td>
      <td>26.08</td>
      <td>8.665</td>
      <td>2</td>
      <td>6</td>
      <td>4</td>
      <td>1.415</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>160</td>
      <td>151</td>
      <td>1</td>
    </tr>
    <tr>
      <th>664</th>
      <td>15652667</td>
      <td>1</td>
      <td>20.00</td>
      <td>0.000</td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>0.500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>144</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>665</th>
      <td>15679622</td>
      <td>1</td>
      <td>31.57</td>
      <td>4.000</td>
      <td>1</td>
      <td>3</td>
      <td>4</td>
      <td>0.085</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>411</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>666</th>
      <td>15730150</td>
      <td>1</td>
      <td>26.75</td>
      <td>4.500</td>
      <td>1</td>
      <td>8</td>
      <td>5</td>
      <td>2.500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>200</td>
      <td>1211</td>
      <td>0</td>
    </tr>
    <tr>
      <th>667</th>
      <td>15813192</td>
      <td>0</td>
      <td>24.92</td>
      <td>1.250</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.000</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>80</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>668</th>
      <td>15606554</td>
      <td>0</td>
      <td>32.25</td>
      <td>1.500</td>
      <td>2</td>
      <td>8</td>
      <td>4</td>
      <td>0.250</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>372</td>
      <td>123</td>
      <td>0</td>
    </tr>
    <tr>
      <th>669</th>
      <td>15611794</td>
      <td>1</td>
      <td>17.67</td>
      <td>4.460</td>
      <td>2</td>
      <td>8</td>
      <td>4</td>
      <td>0.250</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>80</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>670</th>
      <td>15672357</td>
      <td>0</td>
      <td>37.75</td>
      <td>5.500</td>
      <td>2</td>
      <td>11</td>
      <td>4</td>
      <td>0.125</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>228</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>671</th>
      <td>15711759</td>
      <td>1</td>
      <td>22.67</td>
      <td>2.540</td>
      <td>1</td>
      <td>8</td>
      <td>8</td>
      <td>2.585</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>672</th>
      <td>15615296</td>
      <td>0</td>
      <td>17.92</td>
      <td>10.210</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>51</td>
      <td>0</td>
    </tr>
    <tr>
      <th>673</th>
      <td>15699294</td>
      <td>1</td>
      <td>24.42</td>
      <td>12.335</td>
      <td>2</td>
      <td>11</td>
      <td>8</td>
      <td>1.585</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>120</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>674</th>
      <td>15788634</td>
      <td>0</td>
      <td>25.75</td>
      <td>0.500</td>
      <td>2</td>
      <td>8</td>
      <td>8</td>
      <td>0.875</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>491</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>675</th>
      <td>15660871</td>
      <td>1</td>
      <td>26.17</td>
      <td>12.500</td>
      <td>1</td>
      <td>4</td>
      <td>8</td>
      <td>1.250</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>18</td>
      <td>0</td>
    </tr>
    <tr>
      <th>676</th>
      <td>15618258</td>
      <td>0</td>
      <td>22.75</td>
      <td>6.165</td>
      <td>2</td>
      <td>6</td>
      <td>4</td>
      <td>0.165</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>220</td>
      <td>1001</td>
      <td>0</td>
    </tr>
    <tr>
      <th>677</th>
      <td>15722535</td>
      <td>1</td>
      <td>23.00</td>
      <td>0.750</td>
      <td>2</td>
      <td>7</td>
      <td>4</td>
      <td>0.500</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>320</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>678</th>
      <td>15711977</td>
      <td>1</td>
      <td>25.67</td>
      <td>0.290</td>
      <td>1</td>
      <td>8</td>
      <td>4</td>
      <td>1.500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>160</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>679</th>
      <td>15690169</td>
      <td>1</td>
      <td>48.58</td>
      <td>0.205</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>0.250</td>
      <td>1</td>
      <td>1</td>
      <td>11</td>
      <td>0</td>
      <td>2</td>
      <td>380</td>
      <td>2733</td>
      <td>1</td>
    </tr>
    <tr>
      <th>680</th>
      <td>15790689</td>
      <td>1</td>
      <td>21.17</td>
      <td>0.000</td>
      <td>2</td>
      <td>8</td>
      <td>4</td>
      <td>0.500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>681</th>
      <td>15665181</td>
      <td>1</td>
      <td>35.25</td>
      <td>16.500</td>
      <td>1</td>
      <td>8</td>
      <td>4</td>
      <td>4.000</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>80</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>682</th>
      <td>15633608</td>
      <td>0</td>
      <td>22.92</td>
      <td>11.585</td>
      <td>2</td>
      <td>13</td>
      <td>4</td>
      <td>0.040</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>80</td>
      <td>1350</td>
      <td>1</td>
    </tr>
    <tr>
      <th>683</th>
      <td>15805261</td>
      <td>0</td>
      <td>48.17</td>
      <td>1.335</td>
      <td>2</td>
      <td>3</td>
      <td>7</td>
      <td>0.335</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>121</td>
      <td>0</td>
    </tr>
    <tr>
      <th>684</th>
      <td>15740356</td>
      <td>1</td>
      <td>43.00</td>
      <td>0.290</td>
      <td>1</td>
      <td>13</td>
      <td>8</td>
      <td>1.750</td>
      <td>1</td>
      <td>1</td>
      <td>8</td>
      <td>0</td>
      <td>2</td>
      <td>100</td>
      <td>376</td>
      <td>1</td>
    </tr>
    <tr>
      <th>685</th>
      <td>15808223</td>
      <td>1</td>
      <td>31.57</td>
      <td>10.500</td>
      <td>2</td>
      <td>14</td>
      <td>4</td>
      <td>6.500</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>686</th>
      <td>15769980</td>
      <td>1</td>
      <td>20.67</td>
      <td>0.415</td>
      <td>2</td>
      <td>8</td>
      <td>4</td>
      <td>0.125</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>45</td>
      <td>0</td>
    </tr>
    <tr>
      <th>687</th>
      <td>15675450</td>
      <td>0</td>
      <td>18.83</td>
      <td>9.540</td>
      <td>2</td>
      <td>6</td>
      <td>4</td>
      <td>0.085</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>100</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>688</th>
      <td>15776494</td>
      <td>0</td>
      <td>27.42</td>
      <td>14.500</td>
      <td>2</td>
      <td>14</td>
      <td>8</td>
      <td>3.085</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>120</td>
      <td>12</td>
      <td>1</td>
    </tr>
    <tr>
      <th>689</th>
      <td>15592412</td>
      <td>1</td>
      <td>41.00</td>
      <td>0.040</td>
      <td>2</td>
      <td>10</td>
      <td>4</td>
      <td>0.040</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>560</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>690 rows × 16 columns</p>
</div>

<p>데이터를 보시면 첫번째줄은 고객 식별번호id 이고 마지막줄은 카드발급 승인 여부입니다.</p>

<p>이중 마지막 카드발급 승인여부는 따로 y 로 빼둡니다.</p>

<p>Clustering 에 고려할 feature 가 아니기 때문입니다. 그리고 나중에 Clustering 이 잘 되었는지 확인해 볼 때 사용합니다.</p>

<p>데이터를 Min=0, Max=1 이 되도록 Scaling 해줍니다.</p>

<p>불필요한 연산량을 줄이는데 도움됩니다.</p>

<p>sklearn 의 preprocessing 모듈을 이용하면 편리하게 scaling 작업을 하실 수 있습니다.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Feature Scaling</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">])</span> <span class="c">#checking the scaling result of A2</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x11775c048&gt;]
</code></pre>
</div>

<p><img src="output_16_1.png" alt="png" /></p>

<p>그래프를 보시면 0과 1사이의 값으로 잘 Scaling 된것을 보실 수 있습니다.</p>

<p>이제 데이터 준비가 완료되었으니, 준비된 데이터들을 clustering 해봅시다.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Training the SOM</span>
<span class="c">#from minisom import MiniSom</span>
<span class="n">som</span> <span class="o">=</span> <span class="n">MiniSom</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">input_len</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">som</span><span class="o">.</span><span class="n">random_weights_init</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">som</span><span class="o">.</span><span class="n">train_random</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">num_iteration</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
</code></pre>
</div>

<p>만들어놓은 MiniSom model 을 불러오고 Default map 사이즈와 input dimension을 지정해줍니다.</p>

<p>random_weight_init 으로 각 node 들을 초기화시켜준 후</p>

<p>train_random 메서드로 SOM 학습을 시작합니다.</p>

<h2 id="3-확인">3. 확인</h2>

<p>학습이 끝나고 학습의 결과를 pylab 모듈을 통해 확인하겠습니다.</p>

<p>우선 10*10 map 을 평면 위에 표현하고</p>

<p>node 에 최근접한 data point 들이 많을수록 짙게, 적을수록 옅게 표시해줍니다.</p>

<p>Map 의 진한 부분엔 데이터들이 모여있다는 뜻입니다. (Clustering 된것입니다.)</p>

<p><strong><em>따로 빼놓은 y : 카드 발급 여부 데이터를 이제 사용합니다.</em></strong></p>

<p>Map 위에 data point 들에 대응되는 y 를 표현합니다.</p>

<p>data point 와 최근접한 node 위에 data point가 발급 승인된 고객이면 초록네모, 발급 거절된 고객이면 빨간동그라미로 마킹합니다.</p>

<p>진한 node 위에 마킹이 많이 될 것입니다.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Visualizing the results</span>
<span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="n">bone</span><span class="p">,</span> <span class="n">pcolor</span><span class="p">,</span> <span class="n">colorbar</span><span class="p">,</span> <span class="n">plot</span><span class="p">,</span> <span class="n">show</span>
<span class="n">bone</span><span class="p">()</span>
<span class="n">pcolor</span><span class="p">(</span><span class="n">som</span><span class="o">.</span><span class="n">distance_map</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">colorbar</span><span class="p">()</span>
<span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s">'o'</span><span class="p">,</span> <span class="s">'s'</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'r'</span><span class="p">,</span> <span class="s">'g'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">som</span><span class="o">.</span><span class="n">winner</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span>
         <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span>
         <span class="n">markers</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span>
         <span class="n">markeredgecolor</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span>
         <span class="n">markerfacecolor</span> <span class="o">=</span> <span class="s">'None'</span><span class="p">,</span>
         <span class="n">markersize</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
         <span class="n">markeredgewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="output_20_0.png" alt="png" /></p>

<p>이제 결과를 보시면, Map의 진한부분은 데이터가 많이 모인부분, 연한부분은 데이터가 거의 없는 부분이라고 보시면 되는데</p>

<p>이상한 점은 node 가 하얗게 표시된, 연결된 데이터포인트가 거의 없는, 즉 응집이 전혀 없는 부분에 존재하는 데이터들입니다.</p>

<p>저 고객들은 다른 고객들과 전혀 다른 feature 의 구성을 띄고 있어서, 어느 cluster 에도 끼지 못했습니다.</p>

<p>저런 고객들은 수상하죠. 잠재 위험 고객으로 분류합니다.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Finding the frauds</span>
<span class="n">mappings</span> <span class="o">=</span> <span class="n">som</span><span class="o">.</span><span class="n">win_map</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">frauds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">mappings</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)],</span> <span class="n">mappings</span><span class="p">[(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">)]),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">frauds</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">frauds</span><span class="p">)</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">frauds</span><span class="o">.</span><span class="n">shape</span>  <span class="c">#수상한 data 56개 : 다른 데이터들과 거리가 멀다</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(56, 15)
</code></pre>
</div>

<p>수상한 데이터 56개가 검출되었습니다.</p>

<p>아래를 보시면 node(1,4) 에 38개가 수상하고, node(2,9)에 18개가 수상합니다.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mappings</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)])</span><span class="o">.</span><span class="n">shape</span> <span class="c">#(1,4) node 가 winning node(가장가까운노드) 인 data의 갯수 : 38개</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(38, 15)
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mappings</span><span class="p">[(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">)])</span><span class="o">.</span><span class="n">shape</span> <span class="c">#(2,9) node 가 winning node(가장가까운노드) 인 data의 갯수 : 18개</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(18, 15)
</code></pre>
</div>

<p>수상한 고객들의 id 를 뽑아서 확인해보겠습니다.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">frauds_customers_id</span><span class="o">=</span><span class="n">frauds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">frauds_customers_id</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>array([ 15739548.,  15699839.,  15648069.,  15731586.,  15705918.,
        15706762.,  15672894.,  15575605.,  15644453.,  15783883.,
        15756538.,  15694666.,  15728523.,  15646594.,  15593959.,
        15629750.,  15675926.,  15794204.,  15734008.,  15592999.,
        15757434.,  15769548.,  15742009.,  15593834.,  15673907.,
        15737909.,  15599272.,  15660528.,  15779207.,  15711299.,
        15609987.,  15752578.,  15721504.,  15666096.,  15609758.,
        15611682.,  15618258.,  15805261.,  15767729.,  15711446.,
        15720529.,  15787693.,  15688210.,  15704509.,  15793366.,
        15793317.,  15735106.,  15688059.,  15646521.,  15683276.,
        15652289.,  15768777.,  15791326.,  15678779.,  15779586.,
        15730150.])
</code></pre>
</div>

<h1 id="마치며">마치며</h1>

<p>이렇게 SOM 을 간단하게 살펴보았습니다.</p>

<p>궁금한 점 댓글로 남겨주시기 바랍니다. 감사합니다^^.</p>
</div>

    <div class ="container mt-5 mb-5">
  <hr  />
  <div class="row">
    <div class="col-5"> </div>
    <div class="col-2"><img src="/asset/media/image/logo.jpg" width="50" height="50" class="border border-primary px-auto" alt="" style="border-radius:10px; align-center"> </div>
    <div class="col-5"> </div>
  </div>
</div>


    <!--bootstrap Javascript-->
     <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh" crossorigin="anonymous"></script>
     <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ" crossorigin="anonymous"></script>
     <!--Custom Javascript-->
     <script src="/asset/static/post_load.js"></script>
     <script src="/asset/static/post_table_generation.js"></script>
     <script>
  $(document).ready(function() {
    var main_route = (window.location.pathname.split("/")[1]);
    $('#navbar_' + main_route).addClass('active');
    navbar = $('#navbarbackground');
    logotext = $('#logo_text');
    if (main_route == "post"){
      navbar.attr('style',"background-color:rgb(146, 146, 146); background-image:url('/asset/media/image/gradient4.png');   background-blend-mode: color; background-size: cover;");
      logotext.text('post');
      logotext.attr('style',"color:#213b80;");
    }
    else if(main_route == "project"){
      navbar.attr('style',"background-color:rgba(157, 157, 157, 0.54); background-image:url('/asset/media/image/gradient3.jpg');background-blend-mode:color; background-size:cover;");
      logotext.text('project');
      logotext.attr('style',"color:#6f1c16;");
    }
    else if(main_route == "profile"){
      navbar.attr('style',"background-color:rgba(190, 190, 190, 0.75); background-image:url('/asset/media/image/gradient2-1.jpg'); background-blend-mode: color; background-size: cover;");
      logotext.text('profile');
      logotext.attr('style',"color:#6849af;");
    }
    else{
      navbar.attr('style',"background-color : rgba(178, 85, 228, 0.94); background-image:url('/asset/media/image/gradient1.jpg');   background-blend-mode: color; background-size: cover;");
    }
  });
</script>

     <script>
  $(document).ready(function() {
    var main_route = (window.location.pathname.split("/")[2]);
    $('#categorybar_' + main_route).addClass('active').addClass('bg-dark');
  });
</script>

  </body>
</html>
