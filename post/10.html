<!doctype html>
<html>
  <head>

    <title>open-AI 의 gym (python package) 이용해 강화학습 훈련하기 1: Q-learning  | Jayne.who();</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!--bootstrap CSS-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb" crossorigin="anonymous">
    <!--code highlighter CSS for Jekyll Markdown to HTML Converter -->
    <link rel="stylesheet" href="/asset/static/vim-highlight-rouge.css" />
    <link rel="stylesheet" href="/asset/static/post_load.css" />
    <link rel="stylesheet" href="/asset/static/font/stylesheets/NotoSansKR-Hestia.css" />
    <style>
      body {
          font-family: 'Noto Sans Korean', sans-serif;
          font-weight: 500;
          padding-top: 50px;
        }
      h1,h2,h3,h4,h5,h6 {
        font-weight : 800;
        color : #148b8e;
      }
      .navbar, .nav {
        font-weight: 800;
      }
      a {
        color : #90b3d8;
      }
    </style>
  </head>
  <body>

    

<nav id = "navbarbackground" class="navbar fixed-top navbar-expand-lg navbar-light" style="background-color : rgba(178, 85, 228, 0.94); background-image:url('/asset/media/image/gradient1.jpg');   background-blend-mode: color;
  background-size: cover;">
  <a class="navbar-brand" href="/">
    <img src="/asset/media/image/logo.jpg" width="35" height="35" class="d-inline-block align-top border border-primary" alt="" style="border-radius:10px">
    Jayne.who();
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav" style="font-weight : 600;">
      <li class="nav-item" id="navbar_profile">
        <a class="nav-link" href="/profile/">Profile </a>
      </li>
      <li class="nav-item" id="navbar_post">
        <a class="nav-link" href="/post/">Posts </a>
      </li>
      <li class="nav-item" id="navbar_project">
        <a class="nav-link" href="/project/">Projects </a>
      </li>
    </ul>
  </div>
</nav>

    <style>
/*post titles style*/
.post h1,h2,h3,h4,h5,h6 {
  color: #3f71f4;
}
/*for image*/
.post img {
  max-width: 100%;
  height: auto;
}

/*for code highlighter*/
pre {
  background-color: #494b4c ;
  border-radius: 6px;
  padding: 10px;
  color : #d0d7de;
}


</style>

<div class="container-fluid text-center pt-3" style="background-image : url('/asset/media/image/post/10/main.jpg');  background-color: #00010299; background-blend-mode: color; background-size: cover; min-height: 350px;">
  <a href="/post/"><p class="text-left md-5"><button type="button" class="btn btn-outline-secondary btn-sm d-inline text-uppercase"> < Back </button></p></a>
  <h1 class="card-title text-white" style="font-size : 2.3rem;">open-AI 의 gym (python package) 이용해 강화학습 훈련하기 1: Q-learning </h1>
  <p class="card-text text-white"><p class="text-muted">deeplearning | 08 August 2017</p></p>
  <p class="card-text text-white">
    Tags |
    
    <a href="#" class="badge badge-primary">python</a>
    
    <a href="#" class="badge badge-primary">tensorflow</a>
    
    <a href="#" class="badge badge-primary">openai</a>
    
  </p>
</div>


<div class="post container pt-5 " style = "max-width : 750px"><p>open-AI 에서 파이썬 패키지로 제공하는 gym 을 이용하면 , 손쉽게 강화학습 환경을 구성할 수 있다.</p>

<p>gym package 를 이용해서 강화학습 훈련 환경을 만들어보고, Q-learning 이라는 강화학습 알고리즘에 대해 알아보고 적용시켜보자.</p>

<blockquote>

  <h3 id="목차">목차</h3>
  <p><a href="#1.-gym-package-이용하기">1. gym package 이용하기</a></p>

  <p><a href="#2.-Q-learning-이란?">2. Q-learning 이란? </a></p>
  <blockquote>

    <p><a href="#Q-learning-의-학습-(Greedy,-Dummy)">2-1. Q-learning 의 학습(Dummy Q learning)</a></p>

    <p><a href="#Dummy-Q-learning-학습---python-code">2-2. Dummy Q-learning python code</a></p>
  </blockquote>

  <p><a href="#3.-완벽한-Q-learning-(-Dummy-Q-learning-의-문제-)">3. 완벽한 Q-learning ( Dummy Q-learning 의 문제 )</a></p>
  <blockquote>

    <p><a href="#해결책-1-:--E-greedy">3-1. 해결책 1 :  E-greedy </a></p>

    <p><a href="#해결책-2-:-add-Random-noise">3-2. 해결책 2 : add Random Noise</a></p>

    <p><a href="#새로운-문제-:-여러-경로가-생긴다">3-3. 새로운 문제 : 여러 경로 </a></p>

    <p><a href="#Q-learning-python-코드와-실행결과">3-4. 완벽한 Q-learning python code </a></p>
  </blockquote>
</blockquote>

<p><br />
<br /></p>
<h1 id="1-gym-package-이용하기">1. gym package 이용하기</h1>
<hr />

<p><br /></p>

<p>open-AI 에서 만든 gym 이란 파이썬 패키지를 이용하면 강화학습( Reinforcement Learning ) 훈련을 수행할 수 있는 Agent와 Environment 를 제공받을 수 있다.</p>

<blockquote>
  <p><a href="https://gym.openai.com/">open-AI gym 홈페이지</a></p>
</blockquote>

<p>gym 을 간단하게 pip install 통해서 설치할 수 있다.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># command line (bash)</span>
<span class="err">$</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">gym</span>
<span class="err">$</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">readchar</span>
</code></pre>
</div>

<p>실제로 gym 을 사용해본다.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># gym_example.py</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">gym.envs.registration</span> <span class="kn">import</span> <span class="n">register</span>
<span class="kn">import</span> <span class="nn">readchar</span>


<span class="n">LEFT</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">DOWN</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">RIGHT</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">UP</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">arrow_keys</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'</span><span class="se">\x1b</span><span class="s">[A'</span> <span class="p">:</span> <span class="n">UP</span><span class="p">,</span>
    <span class="s">'</span><span class="se">\x1b</span><span class="s">[B'</span> <span class="p">:</span> <span class="n">DOWN</span><span class="p">,</span>
    <span class="s">'</span><span class="se">\x1b</span><span class="s">[C'</span> <span class="p">:</span> <span class="n">RIGHT</span><span class="p">,</span>
    <span class="s">'</span><span class="se">\x1b</span><span class="s">[D'</span> <span class="p">:</span> <span class="n">LEFT</span>
<span class="p">}</span>


<span class="n">register</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="s">'FrozenLake-v3'</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s">"gym.envs.toy_text:FrozenLakeEnv"</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">'map_name'</span><span class="p">:</span><span class="s">'4x4'</span><span class="p">,</span><span class="s">'is_slippery'</span><span class="p">:</span><span class="bp">False</span><span class="p">})</span>


<span class="s">'''여기서부터 gym 코드의 시작이다. env 는 agent 가 활동할 수 있는 environment 이다.'''</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">"FrozenLake-v3"</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span> <span class="c">#환경을 화면으로 출력</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">readchar</span><span class="o">.</span><span class="n">readkey</span><span class="p">()</span>  <span class="c">#키보드 입력을 받는다</span>

    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">arrow_keys</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Game aborted!"</span><span class="p">)</span>
        <span class="k">break</span>

    <span class="n">action</span> <span class="o">=</span> <span class="n">arrow_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="c">#에이젼트의 움직임</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c">#움직임에 따른 결과값들</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span> <span class="c">#화면을 다시 출력</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"State:"</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="s">"Action"</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="s">"Reward:"</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="s">"Info:"</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">done</span><span class="p">:</span> <span class="c">#도착하면 게임을 끝낸다.</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Finished with reward"</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="k">break</span>

</code></pre>
</div>

<p>위와 같은 코드로, 내가 직접 게임을 진행해볼 수 있다.</p>

<p><br />
<br /></p>
<h1 id="2-q-learning-이란">2. Q-learning 이란?</h1>
<hr />

<p><br /></p>

<p><img src="/images/aigym/1.jpeg" alt="" /></p>

<p>위의 gym-example.py 코드같은 environment 에서, agent 가 무작위로 방향을 결정하면 학습이 잘 되지 않는다.</p>

<p>시도 횟수는 엄청 많은데에 비해 reward는 성공할 때 한번만 지급되기 때문이다.</p>

<p>그에 대한 해결책이 바로 Q-learning 이다.</p>

<p>agent는 방향을 결정해야할 때마다 가상의 Q 에게 행보를 물어본다.</p>

<p>Q 는 agent 의 state를 보고 그가 action1 을 취하면 기대되는 reward1(quality1) 을 알려주고, action2 를 취하면 기대되는 reward2(quality2) 를 알려준다.</p>

<p>agent 는 Q의 도움을 받아 더 빠른 학습이 가능하다.</p>

<p>수식적으로는</p>

<p>Q(s, a) = π  처럼 쓸 수 있으며</p>

<p>현재 s(state)에서 취할수 있는 가장 큰 reward 인 max Q  max(Q(s,a)) 로</p>

<p>현재 s(state)에서 max(Q(s,a))로 가게 해주는 action 은 argmax(Q(s,a)) 로 표현한다.</p>

<p>argmax(Q(s,a))는 π*(s) 로도 표현하며 여기서 *은 optimal 함을 의미한다.</p>

<p><img src="/images/aigym/2.jpeg" alt="" /></p>

<h2 id="q-learning-의-학습-greedy-dummy">Q-learning 의 학습 (Greedy, Dummy)</h2>

<p>Q-learning 알고리즘에서 학습한다는 것은 아래 그림처럼 모두 0으로 초기화되어있는 모든 Q 값들을 하나씩 업데이트시킨다는 것과 같다.</p>

<p>모든 칸의 0은 각각의 Q(s,a) 값을 나타낸다.</p>

<p>도착지점으로 넘어갈 때에만 reward = 1 이 주어지고, 나머지 부분으로 넘어갈 때에는 reward = 0 이다.</p>

<p><img src="/images/aigym/6.PNG" alt="" /></p>

<p>우선 각 칸의 Q 값을 업데이트 하는 방법은 다음과같다.</p>

<p>Q(s,a) 의 값은 (다음칸의 Q 중 가장 큰 값 + reward) 로 표현해서 적는다.</p>

<p><img src="/images/aigym/3.jpg" alt="" /></p>

<p>처음에는 어디로 가든 reward 도 0이고 max Q(s’, a’) 도 0이다. 그래서 agent가 무작위로 방향을 결정한다. 그러다 우연히 아래 그림처럼 도착지점 바로 왼쪽칸에 도착했다.</p>

<p>이때 Q(s14, right) 의 값은 reward = 1 값 더하기 maxQ(s15,a’) = 0 으로 1 로 업데이트된다.</p>

<p>이게 첫번째 학습이다.</p>

<p><img src="/images/aigym/5.PNG" alt="" /></p>

<p>한번 학습이 끝난 후 agent 는 다시 시작지점에서 출발한다. 역시나 대부분의 Q가 0으로 초기화되어있고, 주변의 reward 도 0이기 때문에 무작위로 나아간다.</p>

<p>그러다 다시 우연히 s13 칸에 도착했다고 하자.</p>

<p>이때 Q(s13,right) 를 계산해보았더니, reward = 0 이지만 max Q(s’,a’) 이 1이다.</p>

<p>이렇게 다시 Q(s13,right) = 1 로 업데이트하고 다시 agent 를 시작점으로 보낸다.</p>

<p><br />
<br /></p>

<p>이런식으로 계속해서 학습을 하다보면 결국 아래 그림처럼 시작 지점의 Q 까지 학습이 되고, 이렇게 Dummy Q-learning 의 학습이 끝난다.</p>

<p><img src="/images/aigym/3.PNG" alt="" /></p>

<p>루틴을 정리하자면 다음과 같다.</p>

<p><img src="/images/aigym/4.PNG" alt="" /></p>

<h2 id="dummy-q-learning-학습---python-code">Dummy Q-learning 학습 - python code</h2>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># dummy_q_learning.py</span>


<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">gym.envs.registration</span> <span class="kn">import</span> <span class="n">register</span>
<span class="kn">import</span> <span class="nn">random</span> <span class="kn">as</span> <span class="nn">pr</span>


<span class="k">def</span> <span class="nf">qmax_action</span><span class="p">(</span><span class="n">four_q</span><span class="p">):</span>
    <span class="s">""" 상태 s 에서 네가지 a 에 따른 네가지 Q 중 가장 큰 것을 선택 (같으면 랜덤하게 선택)"""</span>
    <span class="n">maxq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">four_q</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">four_q</span> <span class="o">==</span> <span class="n">maxq</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">pr</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>


<span class="n">register</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="s">'FrozenLake-v3'</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s">'gym.envs.toy_text:FrozenLakeEnv'</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">'map_name'</span><span class="p">:</span> <span class="s">'4x4'</span><span class="p">,</span>
            <span class="s">'is_slippery'</span><span class="p">:</span> <span class="bp">False</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'FrozenLake-v3'</span><span class="p">)</span>

<span class="c"># shape = [States num, 4(left,down,right,up)]</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">])</span>
<span class="c"># Set learning parameters</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="c"># create lists to contain total rewards and steps per episode</span>
<span class="n">rList</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="c"># Reset environment and get first new observation</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">rAll</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="c"># The Q-Table learning algorithm</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">qmax_action</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:])</span>

        <span class="c"># Get new state and reward from environment</span>
        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c"># Update Q-Table with new knowledge using learning rate</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">new_state</span><span class="p">,</span> <span class="p">:])</span>

        <span class="n">rAll</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

    <span class="n">rList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rAll</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Success rate: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rList</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_episodes</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Final Q-Table Values"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"LEFT DOWN RIGHT UP"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rList</span><span class="p">)),</span> <span class="n">rList</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><br />
<br /></p>
<h1 id="3-완벽한-q-learning--dummy-q-learning의-문제-">3. 완벽한 Q-learning ( Dummy Q-learning 의 문제 )</h1>
<hr />

<p><br /></p>

<p>dummy Q-learning 의 문제는 아래 그림처럼 가장 optimal 한 경로를 따라 Q 가 업데이트 되지 않을 수 있는 가능성이 있다는 것이다.</p>

<p><img src="/images/aigym/3_1.PNG" alt="" /></p>

<p>해결할 수 있는 방법은 단 하나다.</p>

<p>가끔은 최적의 Q로 이동하는 action 이 아닌 랜덤한 action을 취해주는 것이다.</p>

<p>(예를들어 위 그림에서 첫번째 state 일때, 오른쪽으로 가지 않고 한번 아래로 가보는 action을 취해보는 것이다.)</p>

<p>그 방법으로 2가지가 있다.</p>
<blockquote>

  <ol>
    <li>
      <p>E-greedy (랜덤한 확률로 아무데나 가본다.)</p>
    </li>
    <li>
      <p>add Random noise (Q 에 random noise 를 더해 랜덤한 action 을 취한다.)</p>
    </li>
  </ol>
</blockquote>

<p>위 두가지에 대해 자세히 알아보자.</p>

<h2 id="해결책-1---e-greedy">해결책 1 :  E-greedy</h2>

<p><img src="/images/aigym/7.PNG" alt="" /></p>

<p>일정 확률로 가끔은 최적의 action 을 따라가지 않도록 설정한다.</p>

<h2 id="해결책-2--add-random-noise">해결책 2 : add Random noise</h2>

<p><img src="/images/aigym/9.PNG" alt="" /></p>

<p>action 을 결정할 때 참고하는 각 Q 값에 random 한 noise 를 주어서 action 이 조금 random 해지도록  한다.</p>

<h2 id="새로운-문제--여러-경로가-생긴다">새로운 문제 : 여러 경로가 생긴다</h2>

<p>위와 같이 dummy Q learning 문제를 보완하는 기법들을 사용하면 agent 가 최종적으로 경로를 결정하려고 할 때 선택의 문제에 놓인다.</p>

<p><img src="/images/aigym/10.PNG" alt="" /></p>

<p>이런 상황을 막으려면 학습과정에서 Q를 업데이트할 때, 다음 max Q(s’, a’) 값에다 특정 감마(&lt;1) 값을 곱해준다.</p>

<p>그러면 상대적으로 구불하고 긴 경로로 인도하는 Q 값들은 작아지고, 가장 짧은 경로로 인도하는 Q 값들은 커진다.</p>

<p><img src="/images/aigym/11.PNG" alt="" /></p>

<h2 id="q-learningpython-코드와-실행결과">Q-learning python 코드와 실행결과</h2>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># q_learning.py</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">gym.envs.registration</span> <span class="kn">import</span> <span class="n">register</span>
<span class="kn">import</span> <span class="nn">random</span> <span class="kn">as</span> <span class="nn">pr</span>

<span class="n">register</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="s">'FrozenLake-v3'</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s">'gym.envs.toy_text:FrozenLakeEnv'</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">'map_name'</span><span class="p">:</span> <span class="s">'4x4'</span><span class="p">,</span>
            <span class="s">'is_slippery'</span><span class="p">:</span> <span class="bp">False</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'FrozenLake-v3'</span><span class="p">)</span>

<span class="c"># Initialize table with all zeros</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">])</span>

<span class="s">'''1. Q 값이 업데이트될 때 maxQ(s',a') 에 곱할 감마 값을 설정한다.'''</span>
<span class="n">dis</span> <span class="o">=</span> <span class="o">.</span><span class="mi">99</span>

<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="c"># create lists to contain total rewards and steps per episode</span>
<span class="n">rList</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="c"># Reset environment and get first new observation</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">rAll</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="s">'''2. E-Greedy 를 위한 확률값을 만들어준다. (step i이 지남에 따라 decay 되도록 설정)'''</span>
    <span class="n">e</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">((</span><span class="n">i</span> <span class="o">//</span> <span class="mi">100</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  

    <span class="c"># The Q-Table learning algorithm : 한번 수행할 때 마다 Q 한칸 업데이트</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>

        <span class="s">'''E-Greedy 를 따라 작은 확률로 랜덤하게 가고, 큰 확률로 높은 Q 를 따르는 쪽으로 간다.'''</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:])</span>

        <span class="c"># Get new state and reward from environment</span>
        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c"># Update Q-Table with new knowledge using learning rate</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">dis</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">new_state</span><span class="p">,</span> <span class="p">:])</span>

        <span class="n">rAll</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

    <span class="n">rList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rAll</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Success rate: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rList</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_episodes</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Final Q-Table Values"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rList</span><span class="p">)),</span> <span class="n">rList</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/images/aigym/12.png" alt="" /></p>
</div>

    <div class ="container mt-5 mb-5">
  <hr  />
  <div class="row">
    <div class="col-5"> </div>
    <div class="col-2"><img src="/asset/media/image/logo.jpg" width="50" height="50" class="border border-primary px-auto" alt="" style="border-radius:10px; align-center"> </div>
    <div class="col-5"> </div>
  </div>
</div>


    <!--bootstrap Javascript-->
     <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh" crossorigin="anonymous"></script>
     <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ" crossorigin="anonymous"></script>
     <script src="/asset/static/post_load.js"></script>
     <script>
  $(document).ready(function() {
    var main_route = (window.location.pathname.split("/")[1]);
    $('#navbar_' + main_route).addClass('active');
    navbar = $('#navbarbackground');
    if (main_route == "post"){
      navbar.attr('style',"background-color:rgb(146, 146, 146); background-image:url('/asset/media/image/gradient4.png');   background-blend-mode: color; background-size: cover;");
    }
    else if(main_route == "project"){
      navbar.attr('style',"background-color:rgba(157, 157, 157, 0.54); background-image:url('/asset/media/image/gradient3.jpg');background-blend-mode:color; background-size:cover;");
    }
    else if(main_route == "profile"){
      navbar.attr('style',"background-color:rgba(190, 190, 190, 0.75); background-image:url('/asset/media/image/gradient2-1.jpg'); background-blend-mode: color; background-size: cover;");
    }
    else{
      navbar.attr('style',"background-color : rgba(178, 85, 228, 0.94); background-image:url('/asset/media/image/gradient1.jpg');   background-blend-mode: color; background-size: cover;");
    }
  });
</script>

     <script>
  $(document).ready(function() {
    var main_route = (window.location.pathname.split("/")[2]);
    $('#categorybar_' + main_route).addClass('active').addClass('bg-dark');
  });
</script>

  </body>
</html>
